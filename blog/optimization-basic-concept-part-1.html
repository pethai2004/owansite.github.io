<!doctype html>
<html style="font-size: 16px;"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    <meta name="keywords" content="Post 1 Headline">
    <meta name="description" content="">
    <meta name="page_type" content="np-template-header-footer-from-plugin">
    <title>Optimization basic concept part 1</title>
    <link rel="stylesheet" href="../nicepage.css" media="screen">
<link rel="stylesheet" href="../Post-Template.css" media="screen">
    <script class="u-script" type="text/javascript" src="../jquery.js" defer=""></script>
    <script class="u-script" type="text/javascript" src="../nicepage.js" defer=""></script>
    <meta name="generator" content="Nicepage 4.9.5, nicepage.com">
    <link id="u-theme-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i|Open+Sans:300,300i,400,400i,500,500i,600,600i,700,700i,800,800i">
    
    
    <script type="application/ld+json">{
		"@context": "http://schema.org",
		"@type": "Organization",
		"name": "",
		"logo": "images/default-logo.png",
		"sameAs": []
}</script>
    <meta name="theme-color" content="#478ac9">
  </head>
  <body class="u-body u-xl-mode"><header class="u-clearfix u-header u-header" id="sec-b8de"><div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <a href="https://nicepage.com" class="u-image u-logo u-image-1">
          <img src="../images/default-logo.png" class="u-logo-image u-logo-image-1">
        </a>
        <nav class="u-menu u-menu-dropdown u-offcanvas u-menu-1">
          <div class="menu-collapse" style="font-size: 1rem; letter-spacing: 0px;">
            <a class="u-button-style u-custom-left-right-menu-spacing u-custom-padding-bottom u-custom-top-bottom-menu-spacing u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="#">
              <svg class="u-svg-link" viewBox="0 0 24 24"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#menu-hamburger"></use></svg>
              <svg class="u-svg-content" version="1.1" id="menu-hamburger" viewBox="0 0 16 16" x="0px" y="0px" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><g><rect y="1" width="16" height="2"></rect><rect y="7" width="16" height="2"></rect><rect y="13" width="16" height="2"></rect>
</g></svg>
            </a>
          </div>
          <div class="u-nav-container">
            <ul class="u-nav u-unstyled u-nav-1"><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="../Home.html" style="padding: 10px 20px;">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="../lecture.html" style="padding: 10px 20px;">lecture</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="../project.html" style="padding: 10px 20px;">project</a>
</li></ul>
          </div>
          <div class="u-nav-container-collapse">
            <div class="u-black u-container-style u-inner-container-layout u-opacity u-opacity-95 u-sidenav">
              <div class="u-inner-container-layout u-sidenav-overflow">
                <div class="u-menu-close"></div>
                <ul class="u-align-center u-nav u-popupmenu-items u-unstyled u-nav-2"><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../Home.html">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../lecture.html">lecture</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../project.html">project</a>
</li></ul>
              </div>
            </div>
            <div class="u-black u-menu-overlay u-opacity u-opacity-70"></div>
          </div>
        </nav>
      </div></header>
    <section class="u-align-center u-clearfix u-section-1" id="sec-bf54">
      <div class="u-clearfix u-sheet u-valign-middle-md u-valign-middle-sm u-valign-middle-xs u-sheet-1"><!--post_details--><!--post_details_options_json--><!--{"source":""}--><!--/post_details_options_json--><!--blog_post-->
        <div class="u-container-style u-expanded-width u-post-details u-post-details-1">
          <div class="u-container-layout u-valign-middle u-container-layout-1"><!--blog_post_image-->
            <img alt="" class="u-blog-control u-expanded-width u-image u-image-default u-image-1" src="../images/992f1fab98fffc74d08a2a8558589a9fa1415af2fe7bd838f5ced6aeff4209b17aa411fc25db25b3f5a248f571e83653614f5e9ab945fee345f923_1280.jpg"><!--/blog_post_image--><!--blog_post_header-->
            <h2 class="u-blog-control u-text u-text-1">Optimization basic concept part 1</h2><!--/blog_post_header--><!--blog_post_metadata-->
            <div class="u-blog-control u-metadata u-metadata-1"><!--blog_post_metadata_date-->
              <span class="u-meta-date u-meta-icon">Apr 27, 2022</span><!--/blog_post_metadata_date--><!--blog_post_metadata_category-->
              <!--/blog_post_metadata_category--><!--blog_post_metadata_comments-->
              <!--/blog_post_metadata_comments-->
            </div><!--/blog_post_metadata--><!--blog_post_content-->
            <div class="u-align-justify u-blog-control u-post-content u-text u-text-2 fr-view"><p id="isPasted">NUMERICAL OPTIMIZATION LECTURE 1</p><p>Review Positive definite matrices</p><ul><li>A matrix is positive definite if it’s symmetric and all its eigenvalues are positive.</li><li>A matrix is positive definite if it’s symmetric and all its pivots are positive.</li></ul><p>Some useful theorems:</p><ul><li>Taylor’s theorem (main)</li></ul><p><br></p><p align="center" style="margin-left:18.0pt;text-align:center;"><br></p><p align="center" style="margin-left:18.0pt;text-align:center;"><br></p><ul><li>First-order necessary conditions state that if &nbsp;is loc minimum then &nbsp;(stationary point).</li><li>Second-order necessary conditions state that if &nbsp;is loc minimum and if &nbsp;and &nbsp;exist then &nbsp;and &nbsp;is positive semidefinite.</li><li>Second-order sufficient conditions, follow from 3rd, if &nbsp;is smooth then &nbsp;is strong minimizer (strong minimizer: for any neighborhood N of , &nbsp;for all &nbsp;in N)</li><li>Theorem 5, if function is convex then any local minimum is also global.</li></ul><p><br></p><p>Overview: Line search and Trust Region Method</p><ul><li>Line search method</li></ul><p>In line search we fix the direction search and find appropriate distance of the next move iteration. &nbsp;Possible search directions for example are:</p><ul><li>steepest descent</li></ul><p>It’s best to choose search direction that f is decreasing most rapidly, it moves along the direction . Recall the directional derivative is given by .</p><p>Theorem: The maximum value of &nbsp;(hence the maximum rate of ) is given by &nbsp;in direction given by .</p><p>Since (from fact about dot product)</p><p align="center" style="text-align:center;"><br></p><p>And min value of &nbsp;is -1 and occur when angle between &nbsp; and &nbsp;is 180, it’s pointing the same direction as . Thus, the direction of min rate of change of &nbsp;follow the direction with . *Note irrelevant fact, for 3 variables, &nbsp;is orthogonal to level surface. Also for maximum direction we could just think max value of &nbsp;is 0 with direction .</p><p>Here direction &nbsp;is a unit vector:</p><p align="center" style="text-align:center;"><br></p><ul><li>descent direction</li></ul><p>Simply make angle strictly less than &nbsp;and if direction &nbsp;is point downhill, then &nbsp;and .</p><p><br></p><ul><li>newton direction</li></ul><p>Newton direction which is derived from second-order Taylor series and can be use when &nbsp;is positive definite. We use approximation:</p><p><br></p><p>We obtain Newton direction by setting &nbsp;to zero and obtain</p><p><br></p><p>Note* the different term of 3rd in 2.6 is negligible if &nbsp;note too large from .</p><p>Note that Newton direction is also descent direction, however most implementation use step size of 1 and adjust only when objective don’t decrease. Also if &nbsp;note positive definite then its inverse do not exist and cannot apply the direction equation , however, we can modify this definition of &nbsp;while benefit from &nbsp;(second order information). It’s computational expensive to compute for Hessian, we can use finite-difference and automatic diff to taggle this.</p><p><br></p><ul><li>Quasi-Newton</li></ul><p>Quasi-Newton approximates hessian in different way. We write, from Taylor’ theorem,</p><p>,</p><p>and setting &nbsp;and , we have ( size of final integral is o( .)</p><p>.</p><p>We approximate a new Hessian &nbsp;that mimics the property &nbsp;and satisfied secant equation: , where &nbsp;and .</p><p>We typically condition on &nbsp;for example, symmetry (exact Hessian is symmetry) and two successive &nbsp;and &nbsp;have low rank. Two popular formulae for updating Hessian are symmetric-rank-one and BFGS. We then can replace approximate in newton direction as</p><p>.</p><p>In practice we can update inverse of Hessian instead of the need to factorize/back-substitution of stated updating formula, we applied &nbsp; and use .</p><p><br></p><p>In summary, often search direction has the form of ; where &nbsp;is identity for steepest descent, exact Hessian for Newton’s method and approximate Hessian for quasi-Newton.</p><ul><li>nonlinear conjugate gradient method</li></ul><p>Originally designed to solve linear system of &nbsp;and is equivalent to minimizing convex quadratic function of . This method has the from where &nbsp;ensue that &nbsp;and &nbsp;is conjugate.</p><p><br></p><p><br></p><ul><li>trust region method</li></ul><p>In trust region we choose maximum search distance, radius for example and seek the direction.</p><p>We gathered information in neighborhood of &nbsp;by the model function &nbsp;which is similar in near current point . In other word, we solve subproblem lied withing this :</p><p>, where &nbsp;lies inside the region.</p><p>&nbsp;is usually defined to be quadratic: where &nbsp;is Hessian or its approximation.</p><p><br></p><p>Line search Method chapter 3</p><p>General update rule is given by .</p><ul><li>step size</li></ul><p>One would choose an ideal of step size that produce most reduction in , or simply the ideal is to choose global minimizer of . &nbsp;This is an expensive evaluation. Inexact line search tries out candidate of step length &nbsp;and terminate when conditions is satisfied. In exact line search done in two stages: bracketing finds an interval of step lengths, and interpolation or bisection computes good step length. We simply condition that .</p><p><br></p><ul><li>Wolfe Condition</li></ul><p>Sufficient decrease in &nbsp;is measure by , where . It state that &nbsp;is acceptable only if . Second condition, the curvature condition, ensure that , where &nbsp;and initial slope . It requires &nbsp;to satisfy . The difference to strong Wolfe condition is not allow &nbsp;to be too positive. We exclude point far from stationary points of &nbsp;and force it close to its critical point. &nbsp;</p><p>Note that only first condition is not ensure progress in optimizing and need 2nd &nbsp;condition, however, we can dispense the 2nd condition using backtracking. It is suited for Newton methods but less for quasi-Newton and conjugate gradients. (see more on Backtracking Line Search)</p><p><br></p><ul><li>Goldstein Condition</li></ul><p><br></p><ul><li>Convergence analysis</li></ul><p>Define .</p><p>Zoutendijk condition</p><p align="center" style="text-align:center;"><br></p><p>Zoutendijk condition implies that , this limit cat be used to derive global convergence results for line search algorithms. (p.58) We can be sure that &nbsp;provided that search direction are never close to orthogonality with gradient. If line search we have chosen ensure that there exist &nbsp;such that , for all k and that from (3.14) . In steepest descent, search direction &nbsp;is parallel to negative gradient , and that , sequence of gradient converges to zeros provided that it use a line search satisfied Wolfe or Goldstein conditions. However, we cannot guarantee converges of minimizer only that it will converges to stationary points.</p><p><br></p><p>Key property of designing algorithm is that we need to ensure that search direction &nbsp;does not tend to orthogonal to gradient &nbsp;since, intuitively, can be thought like we circle around the mountain but never go upward (max) or downward (min), or steepest descent are taken too regularly. We could compute &nbsp;at every iteration and turn &nbsp;toward steepest direction if &nbsp;. But this are undesirable, inappropriate &nbsp;may slow down convergence rate (ill-conditioned Hessian, rapid change in ) that may necessary to produce &nbsp;almost orthogonal to gradient. And angle tests destroy the invariance property of quasi-newton method.</p><p><br></p><ul><li>Rate of Convergence</li></ul><p>Recall: if &nbsp;is positive definite (all eigenvalues are positive) at , then x is local minimum of .</p><p>if &nbsp;is negative definite (all eigenvalues are negative) at , then x is local maximum of .</p><p>If &nbsp;is a mix at , then x is saddle point of , and if 0 then the test is inconclusive.</p><p><br></p><ul><li>Steepest descent</li></ul><p>We cannot expect the rate of convergence to improve if an inexact line search is used. (p.44) Even Hessian is well conditioned, steepest descent can have a slow convergence.</p><ul><li>Newton’s method</li></ul><p>Consider search direction , its Hessian may not be positive definite therefore may note follow the descent direction.</p><p><br></p><ul><li>Newton’s method with Hessian modification</li></ul><p align="center" style="text-align:center;"><br></p><p>Equation &nbsp;can be solve using Gaussian elimination. Hessian &nbsp;is obtained by either adding positive diagonal matrix or full matrix. We obtain global convergence provided that &nbsp;have bound condition number whenever &nbsp;is bounded: , some &nbsp;and all k = 0,1,2, … We would like to modify as small as possible to preserve the Hessian information.</p><p><br></p><p>Suppose sequence of iteration converges where &nbsp;is sufficiently positive definite, modification . We have that &nbsp;for large k and algorithm 3.2 reduce to pure newton and have quadratic convergence rate. If &nbsp;close to singular, no guarantee that &nbsp;will vanish and convergence rate may only linear.</p><p><br></p><p>Example: modified Cholesky factorization</p><p>This approach modify Hessian that is not positive definite by factorizing &nbsp;and increase the diagonal elements during factorization. It’s design for two goals: guarantee that &nbsp;factorization exist and bounded relative to norm of actual Hessian, and live actual Hessian unchanged if it’s already positive definite. Technical treatment can be found in (p.53 numerical optimization)</p><p><br></p><ul><li>Step-length selection algorithm</li></ul><p>Consider finding minimum of . All line search procedures require initial estimate &nbsp;, and it has two steps: bracketing that finds interval of acceptable step lengths, and selection that zoom in to desired step length and interpolates some of function and derivative gathered on earlier step to guess the minimizer location.</p><ul><li>Interpolation</li></ul><p>Our goal is to find value of &nbsp;that satisfies sufficient decrease condition (1st Wolfe condition) can be writing as</p><p>&nbsp;without being too small. (Note &nbsp;and , in practice) (full description can be found at p.58).</p><ul><li>Initial step length</li></ul><p>For Newton and quasi-Newton method, the initial step should always be 1. This ensure unit step lengths are taken whenever they satisfy the termination conditions and allow fast rate of convergence property to effect. (Numerical opt)</p><p>For methods such as steepest descent and conjugate gradients are not produce will scaled search directions. It’s important to use information about the problem and algorithm. Example strategy: is choose &nbsp;so that , change in function at k is the same as that obtained from k-1.</p><p><br></p><p>First, trial estimate &nbsp;keeps increasing it until find acceptable length or interval of desired lengths.</p><p>Second, decrease size of interval until acceptable length is identified, done by zoom function (algorithm).</p><p>Note that line search terminates with &nbsp;whenever step length satisfies strong Wolfe conditions. (full treatment can be found at p.61) Line search can globally converge even step length is not optimal, it only require to satisfies loose criteria.</p><p><br></p><p>Trust Region Methods chapter 4</p><p>We solve another subproblem: &nbsp;which is approximate of &nbsp;near local &nbsp;and define by Taylor expansion in (2.6) as &nbsp;where &nbsp;is approximate of Hessian in the third term of (2.6). We want to find direction &nbsp;that minimize . The key is to choose appropriate step size which is the radius of trust region . Note that calculate &nbsp; is not costly as actual . We base this choice on &nbsp;and &nbsp;of previous iteration, define</p><p><br></p><p><br></p><p>Is always nonnegative, if not then &nbsp;so we reject the step. If &nbsp;close to 1 then approximation (ratio) of &nbsp;and &nbsp;is similar to each other and we appropriately expand size of region . And we do the opposite if it close to zero or negative-reduce the region at next iteration. Usually &nbsp;if, for some constant &nbsp;(typically set to 0.00001), &nbsp;otherwise .</p><p align="center" style="text-align:center;"><br></p><p><br></p><ul><li>Cauchy Point</li></ul><p>Find &nbsp; satisfies , linear version of &nbsp;subject to . The solution is in the closed form given by . Then, find &nbsp;satisfies &nbsp;subject to .</p><p>We obtain Cauchy point that yield sufficient reduction in &nbsp;as , where</p><p>. (I don’t understand this)</p><p>Cauchy Point only use Hessian &nbsp;for determine the step length not direction. We can design to take full step &nbsp;if Hessian is positive definite and . Since taking Cauchy point as a step is like steepest descent, and steepest descent performs poorly even optimal step length is taken. (see p.91)Dogleg method is one such approach to approximate subproblem minimization .</p><p>Conjugate Gradient chap 5</p><p>Conceptual:</p><p>It is an iterative method for solving systems of linear equation where A is positive definite coefficient. The performance of conjugate gradient is determined by distribution of eigenvalues of matrix &nbsp;and can be preconditioned to make it more efficient for improving convergence.</p><p>Some Advantages:</p><ul><li>Less memory consumption since it require no matrix storage</li><li>Faster convergence than descent method</li></ul><p><br></p><p>Solving is equivalent to solving minimization of convex quadratic functions:</p><p>.</p><p>This can be seen easily by differentiate &nbsp;and set it to zero and we obtain the similar solution. In other word, the gradient &nbsp;is the residual of the linear system: , satisfies &nbsp;. We can measure how much solution &nbsp;fails to satisfy &nbsp;by relative residual: . Conjugate gradient generate sets of vectors with a conjugacy property. *A set of nonzero vectors is said to be conjugate with respect to the symmetric positive definite matrix &nbsp;If &nbsp;for all . This set of vectors is also linearly independent. Geometrically, the conjugate direction is a pair of directions; curve and the characteristic of line of tangent plane (in 3 dim).</p><p>We update , where &nbsp;is one-dimensional minimizer off quadratic function &nbsp;along , given by . This can be shown by simply differentiating and set it to zero then solve for : &nbsp; (exercise 3.3). We arrive at following theorem:</p><p><br></p><p align="center" style="text-align:center;"><br></p><p>One property that allow Conjugate method to store less memory is that in generating conjugate set , it only require to use previous &nbsp;not entire generated set, it automatically conjugate to these sets. Therefore, less storage and computation.</p><p>Note: In the book (numerical optimization) define residual as . Thus the search direction is linear combination of negative residual &nbsp;and , given by . And (5.14d) can be obtained by multiplying &nbsp;by &nbsp;and use the conjugacy property: . First search direction &nbsp;is chosen to be descent direction. Theorem 5.3 (5.16) suggest that residuals &nbsp;are mutually orthogonal. Each search direction &nbsp;and residual &nbsp;is &nbsp;contained in Krylov subspace of degree k for &nbsp;defined as .</p><p style="text-align:justify;"><br></p><p>** More practical form can be obtained by replacing (5.14a) by &nbsp; and (5.14d) by . (see p. 111).</p><ul><li>Preconditioning</li></ul><p>Key idea is to transform &nbsp;to , where &nbsp;is nonsingular (i.e., Zero det and invertible). And transform quadratic &nbsp;by &nbsp;and is equivalent to solve the linear system . The convergence rate depend on eigenvalues of &nbsp;and we choose &nbsp;such that the eigenvalues are more favorable (see p. 112 for theoretical analysis of convergence rate). We can choose &nbsp;such that condition number is smaller than original one or such that the eigenvalues of transformed matrix are clustered. It unnecessary to use direct transformation, rather, we apply &nbsp;in algorithm then revert equation in terms of original .</p><p align="center" style="text-align:center;"><br></p><p>The matrix &nbsp;do not use , but rather , which is symmetric and positive definite by construction. If , it reduces to standard CG. The orthogonal property of &nbsp;becomes , for all . Some general precondition strategies include (SSOR), incomplete Cholesky, and banded preconditioners. In general, there is no exact strategy that is best, it’s depend on problem.</p><ul><li>Nonlinear CG method</li></ul><p>This apply to problem of minimizing general convex functions or nonlinear functions. This is well studied method and proved to be quite successful. Some method include:</p><ul><li>Fletcher-Reeves Method</li><li>Polak-Ribiere method and its variants</li><li>Quadratic termination and Restarts</li></ul></div><!--/blog_post_content-->
          </div>
        </div><!--/blog_post--><!--/post_details-->
      </div>
    </section>
    
    
    <footer class="u-clearfix u-footer u-grey-80" id="sec-e381"><div class="u-clearfix u-sheet u-sheet-1">
        <a href="https://nicepage.com" class="u-image u-logo u-image-1" data-image-width="80" data-image-height="40">
          <img src="../images/default-logo-5.png" class="u-logo-image u-logo-image-1">
        </a>
        <div class="u-align-left u-social-icons u-spacing-10 u-social-icons-1">
          <a class="u-social-url" title="facebook" target="_blank" href=""><span class="u-icon u-social-facebook u-social-icon u-icon-1"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-9909"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-9909"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M73.5,31.6h-9.1c-1.4,0-3.6,0.8-3.6,3.9v8.5h12.6L72,58.3H60.8v40.8H43.9V58.3h-8V43.9h8v-9.2
            c0-6.7,3.1-17,17-17h12.5v13.9H73.5z"></path></svg></span>
          </a>
          <a class="u-social-url" title="twitter" target="_blank" href=""><span class="u-icon u-social-icon u-social-twitter u-icon-2"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-d07a"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-d07a"><circle fill="currentColor" class="st0" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M83.8,47.3c0,0.6,0,1.2,0,1.7c0,17.7-13.5,38.2-38.2,38.2C38,87.2,31,85,25,81.2c1,0.1,2.1,0.2,3.2,0.2
            c6.3,0,12.1-2.1,16.7-5.7c-5.9-0.1-10.8-4-12.5-9.3c0.8,0.2,1.7,0.2,2.5,0.2c1.2,0,2.4-0.2,3.5-0.5c-6.1-1.2-10.8-6.7-10.8-13.1
            c0-0.1,0-0.1,0-0.2c1.8,1,3.9,1.6,6.1,1.7c-3.6-2.4-6-6.5-6-11.2c0-2.5,0.7-4.8,1.8-6.7c6.6,8.1,16.5,13.5,27.6,14
            c-0.2-1-0.3-2-0.3-3.1c0-7.4,6-13.4,13.4-13.4c3.9,0,7.3,1.6,9.8,4.2c3.1-0.6,5.9-1.7,8.5-3.3c-1,3.1-3.1,5.8-5.9,7.4
            c2.7-0.3,5.3-1,7.7-2.1C88.7,43,86.4,45.4,83.8,47.3z"></path></svg></span>
          </a>
          <a class="u-social-url" title="instagram" target="_blank" href=""><span class="u-icon u-social-icon u-social-instagram u-icon-3"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-808b"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-808b"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M55.9,38.2c-9.9,0-17.9,8-17.9,17.9C38,66,46,74,55.9,74c9.9,0,17.9-8,17.9-17.9C73.8,46.2,65.8,38.2,55.9,38.2
            z M55.9,66.4c-5.7,0-10.3-4.6-10.3-10.3c-0.1-5.7,4.6-10.3,10.3-10.3c5.7,0,10.3,4.6,10.3,10.3C66.2,61.8,61.6,66.4,55.9,66.4z"></path><path fill="#FFFFFF" d="M74.3,33.5c-2.3,0-4.2,1.9-4.2,4.2s1.9,4.2,4.2,4.2s4.2-1.9,4.2-4.2S76.6,33.5,74.3,33.5z"></path><path fill="#FFFFFF" d="M73.1,21.3H38.6c-9.7,0-17.5,7.9-17.5,17.5v34.5c0,9.7,7.9,17.6,17.5,17.6h34.5c9.7,0,17.5-7.9,17.5-17.5V38.8
            C90.6,29.1,82.7,21.3,73.1,21.3z M83,73.3c0,5.5-4.5,9.9-9.9,9.9H38.6c-5.5,0-9.9-4.5-9.9-9.9V38.8c0-5.5,4.5-9.9,9.9-9.9h34.5
            c5.5,0,9.9,4.5,9.9,9.9V73.3z"></path></svg></span>
          </a>
          <a class="u-social-url" title="linkedin" target="_blank" href=""><span class="u-icon u-social-icon u-social-linkedin u-icon-4"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-694a"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-694a"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M41.3,83.7H27.9V43.4h13.4V83.7z M34.6,37.9L34.6,37.9c-4.6,0-7.5-3.1-7.5-7c0-4,3-7,7.6-7s7.4,3,7.5,7
            C42.2,34.8,39.2,37.9,34.6,37.9z M89.6,83.7H76.2V62.2c0-5.4-1.9-9.1-6.8-9.1c-3.7,0-5.9,2.5-6.9,4.9c-0.4,0.9-0.4,2.1-0.4,3.3v22.5
            H48.7c0,0,0.2-36.5,0-40.3h13.4v5.7c1.8-2.7,5-6.7,12.1-6.7c8.8,0,15.4,5.8,15.4,18.1V83.7z"></path></svg></span>
          </a>
        </div>
        <div class="u-border-1 u-border-white u-expanded-width u-line u-line-horizontal u-opacity u-opacity-50 u-line-1"></div>
        <p class="u-align-center u-text u-text-1">Sample text. Click to select the text box. Click again or double click to start editing the text.</p>
      </div></footer>
    <section class="u-backlink u-clearfix u-grey-80">
      <a class="u-link" href="https://nicepage.com/website-templates" target="_blank">
        <span>Website Templates</span>
      </a>
      <p class="u-text">
        <span>created with</span>
      </p>
      <a class="u-link" href="" target="_blank">
        <span>Website Builder Software</span>
      </a>. 
    </section>
  
</body></html>
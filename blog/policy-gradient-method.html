<!doctype html>
  <script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<html style="font-size: 16px;" lang="en"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    <meta name="keywords" content="Post 1 Headline">
    <meta name="description" content="">
    <title>Policy Gradient Method</title>
    <link rel="stylesheet" href="../nicepage.css" media="screen">
<link rel="stylesheet" href="../Post-Template.css" media="screen">
    <script class="u-script" type="text/javascript" src="../jquery.js" defer=""></script>
    <script class="u-script" type="text/javascript" src="../nicepage.js" defer=""></script>
    <meta name="generator" content="Nicepage 4.14.1, nicepage.com">
    <link id="u-theme-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i|Open+Sans:300,300i,400,400i,500,500i,600,600i,700,700i,800,800i">
    
    
    <script type="application/ld+json">{
		"@context": "http://schema.org",
		"@type": "Organization",
		"name": "",
		"logo": "images/owanlogo.png",
		"sameAs": [
				"https://www.facebook.com/owan.naruemit",
				"https://twitter.com/napanaruemit",
				"https://www.linkedin.com/in/pethai-napanaruemit-3a69a7190/",
				"https://github.com/pethai2004",
				"https://www.youtube.com/channel/UCkQZQFB-PPBgNEL-GizSucw"
		]
}</script>
    <meta name="theme-color" content="#478ac9">
    <meta name="twitter:site" content="@">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Post Template">
    <meta name="twitter:description" content="">
  </head>
  <body class="u-body u-xl-mode" data-lang="en"><header class="u-clearfix u-header u-palette-1-light-2 u-header" id="sec-b8de"><div class="u-clearfix u-sheet u-sheet-1">
        <a href="../Home.html" data-page-id="208602995" class="u-image u-logo u-image-1" data-image-width="1000" data-image-height="1000" title="Home">
          <img src="../images/owanlogo.png" class="u-logo-image u-logo-image-1">
        </a>
        <nav class="u-menu u-menu-dropdown u-offcanvas u-menu-1">
          <div class="menu-collapse" style="font-size: 1rem; letter-spacing: 0px; font-weight: 700;">
            <a class="u-button-style u-custom-left-right-menu-spacing u-custom-padding-bottom u-custom-top-bottom-menu-spacing u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="#">
              <svg class="u-svg-link" viewBox="0 0 24 24"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#menu-hamburger"></use></svg>
              <svg class="u-svg-content" version="1.1" id="menu-hamburger" viewBox="0 0 16 16" x="0px" y="0px" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><g><rect y="1" width="16" height="2"></rect><rect y="7" width="16" height="2"></rect><rect y="13" width="16" height="2"></rect>
</g></svg>
            </a>
          </div>
          <div class="u-custom-menu u-nav-container">
            <ul class="u-nav u-unstyled u-nav-1"><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="../Home.html" style="padding: 10px 20px;">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="../project-and-code.html" style="padding: 10px 20px;">project and code</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="../lecture.html" style="padding: 10px 20px;">lecture</a>
</li></ul>
          </div>
          <div class="u-custom-menu u-nav-container-collapse">
            <div class="u-black u-container-style u-inner-container-layout u-opacity u-opacity-95 u-sidenav">
              <div class="u-inner-container-layout u-sidenav-overflow">
                <div class="u-menu-close"></div>
                <ul class="u-align-center u-nav u-popupmenu-items u-unstyled u-nav-2"><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../Home.html">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../project-and-code.html">project and code</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../lecture.html">lecture</a>
</li></ul>
              </div>
            </div>
            <div class="u-black u-menu-overlay u-opacity u-opacity-70"></div>
          </div>
        </nav>
      </div></header>
    <section class="u-align-center u-clearfix u-section-1" id="sec-bf54">
      <div class="u-clearfix u-sheet u-valign-middle-md u-valign-middle-sm u-valign-middle-xs u-sheet-1"><!--post_details--><!--post_details_options_json--><!--{"source":""}--><!--/post_details_options_json--><!--blog_post-->
        <div class="u-container-style u-expanded-width u-post-details u-post-details-1">
          <div class="u-container-layout u-valign-middle u-container-layout-1", style= "position: center;padding: 0px 100px 5px 100px;"><!--blog_post_image-->
            <!--/blog_post_image--><!--blog_post_header-->
            <h2 class="u-blog-control u-text u-text-1">Policy Gradient Method</h2><!--/blog_post_header--><!--blog_post_metadata-->
            <div class="u-blog-control u-metadata u-metadata-1"><!--blog_post_metadata_date-->
              <span class="u-meta-date u-meta-icon">May 29, 2022</span><!--/blog_post_metadata_date--><!--blog_post_metadata_category-->
              <!--/blog_post_metadata_category--><!--blog_post_metadata_comments-->
              <!--/blog_post_metadata_comments-->
            </div><!--/blog_post_metadata--><!--blog_post_content-->
            <div style= "position: center;">

<p>
The objective in general formulation of policy gradient is maximizing the expectation over possible trajactory:
\begin{equation}
f=\mathbb{E}_{\tau \sim p( \tau )}[ R( \tau )] =\int _{\tau } p( \tau ) R( \tau ) d\tau 
\end{equation}
This can be approximated by running multiple trajactory and then average over them to form Monte-Carlo sampling: \( f\approx 1/N\sum _{i=1}^{N} R( \tau _{i})\). The very generic update form is gradient ascent: \( \theta _{k+1}\leftarrow \theta _{k} +\alpha \nabla _{\theta } f\). In addition to objective \( f\), we can introduce exploration by adding entropy of the action: \( f_{H} =f+\beta \mathbb{H}( \pi )\), recall that  entropy \( \mathbb{H}\) measure an information of random variable, the more \( \mathbb{H}\) the less deterministic the action. Note that RL assume the state distribution \( p( \tau )\) is according to makov stateless property, so \( p( \tau )\) is given by:
\begin{equation}
p( \tau ) =p( s_{0} ,a_{0} ...,s_{T} ,a_{T}) =p( s_{0})\prod _{t=0}^{T} \pi _{\theta }( a_{t} |s_{t}) p( s_{t+1} |s_{t} ,a_{t})
\end{equation}
What nice is that the gradient \( \nabla _{\theta } f\) can be written as \( \nabla _{\theta } f=\mathbb{E}_{\tau \sim p( \tau )}[ R( \tau ) \nabla _{\theta }\log p( \tau )]\), meaning that we can also use MC approach for approximate the gradient. Now the gradient of log-likelihood of trajactory can be written as \( \nabla _{\theta }\log p( \tau ) =\sum _{t=1}^{T} \nabla _{\theta }\log \pi _{\theta }( a_{t} |s_{t})\) and it is independent of state distribution. In other word, in the view of formulation of Markovian dynamical system, we do not have to access the true distribution of state but still can compute the gradient \( \nabla _{\theta } f\). This is neccesary since, imagine integrating over entire state space, don't think about it. This enable the model-free method where state distribution is not known. To summarise, the approximate \( \nabla _{\theta } f\) can be approximate as
\begin{equation}
\nabla _{\theta } f=\frac{1}{N}\sum _{i=1}^{N}\sum _{t=1}^{T} R_{i} \nabla _{\theta }\log \pi _{\theta }( a_{t} |s_{t})
\end{equation}
</p>
<p>
There's papers proposed variance reduction by subtracting baseline and proved to be unbiases as long as some constant, \( b\) do not depend on parameters \( \theta \) (want to try this: make \( b\) depend on \( \theta \)). The baseline can be replace by approximate advantage \( A( s,a)\):
1. Monte Carlo \( A( s,a) =R( s,a) -V_{\theta }( s)\), rely on full episode before update
2. Temperal Different \( A( s,a) =r+\gamma V_{\theta }\left( s^{'}\right) -V( s)\), update every step and can be unstable
3. N-step \( A( s,a) =\sum _{n=1}^{N} \gamma ^{n-1} r_{t+n} +\gamma ^{n} V_{\theta }( s_{t+n}) -V_{\theta }( s)\), something between the former extreme
4. GAE: estimating Advantage as \( A_{t}^{GAE( \lambda ,\gamma )}( s,a) =\sum _{l=0}^{\infty }( \gamma \lambda )^{l} \delta _{t+l}\),
Actor-Critic
Instead of using return (MC rollouts) in the episode, AC estimate it using temporal different (TD) learning. It replace \( G_{t}\) with \( G_{t:t+1} =r_{t} +\gamma V_{w}( s_{t+1})\). 
</p>

<h4>Off-policy gradient Method [more]</h4>
<p>
The key different to on-policy is it use policy generating the trajactory (behavior) different from the updating policy (target), this give us the use of importance sampling in off-policy. In naive approach would be compute objective \( f_{\beta }\) and compute its gradient but, if writing explicitly it is not the same of what is objective of interest:
\begin{equation}
\int _{\tau } p_{\theta }( \tau ) R( \tau ) d\tau \neq \int _{\tau } p_{\beta }( \tau ) R( \tau ) d\tau 
\end{equation}
What we want is to make expectation over \( p_{\theta }( \tau )\) while sampling from \( \beta \). Deriving from importance sampling technique, the objective is then:\begin{equation}
\mathbb{E}_{\tau \sim p_{\theta }( \tau )}[ R( \tau )] =\mathbb{E}_{\tau \sim p_{\beta }( \tau )}\left[\frac{p_{\theta }( \tau )}{p_{\beta }( \tau )} R( \tau )\right]
\end{equation}From definition of \( p\), the ratio can be written as \( \prod _{t=0}^{T}\frac{\pi _{\theta }( \tau )}{\beta _{\beta }( \tau )}\), it does not depend on state distribution but only policies and can be estimate using Monte Carlo as before. To derive the gradient, first the objective:
\begin{gather}
f( \theta ) =\mathbb{E}_{s\sim p^{\beta }( s) ,a\sim \pi _{\theta }( a|s)}[ Q_{\pi }( s,a)] =\sum _{s\in S} p^{\beta }( s)\sum _{a\in A} \pi _{\theta }( a|s) Q_{\pi }( s,a)\\
\nabla _{\theta } f=\mathbb{E}_{s\sim d}\left[\sum _{a\in A}[ \nabla _{\theta } \pi _{\theta }( a|s) Q_{\pi }( s,a) +\pi _{\theta }( a|s) \nabla _{\theta } Q_{\pi }( s,a)]\right]\\
=\mathbb{E}_{s\sim d}\left[\sum _{a\in A} \nabla _{\theta } \pi _{\theta }( a|s) Q_{\pi }( s,a)\right] =\mathbb{E}_{s\sim d}\left[\sum _{a\in A} \nabla _{\theta } \pi _{\theta }( a|s)\frac{\pi _{\theta }( a|s)}{\beta ( a|s)} Q_{\pi }( s,a)\frac{\beta ( a|s)}{\pi _{\theta }( a|s)}\right]
\end{gather}
\begin{equation}
=\mathbb{E}_{s\sim d,a\sim \beta }\left[\sum _{a\in A}\frac{\pi _{\theta }( a|s)}{\beta ( a|s)} Q_{\pi }( s,a) \nabla _{\theta }\log \pi _{\theta }( a|s)\right]
\end{equation}
Note that the second term of gradient \( \nabla _{\theta } f\) is negligible in gradient approximation and proof to produce improvement.
</p>
<h4>Deterministic Policy Gradient (DPG) [papers|original code] </h4>
<p>
Assuming that the action \( a\in A\) is continuous and is deterministic function of the policy, \( a_{t} =\mu _{\theta }( s_{t})\) rather than a stochastic policy, \( a_{t} \sim \pi _{\theta }( s_{t})\). The objective is to maximize the \( Q\)-function:
\begin{equation}
f=\mathbb{E}_{s\sim p( s)}[ Q( s,a=\pi _{\theta }( s_{t}))]
\end{equation}
The DPG can be thing like a stochatic policy where action \( a\) is distributed according to delta function where there's a "spike" in one value. In DPG papers shows that it is equivalent for stochastic policy when reparametrized by deterministic policy, the variance, \( \mu \) is zero. The computed gradient is : 
\begin{equation}
\nabla _{\theta } f( \mu _{\theta }) =\mathbb{E}_{s\sim p( s)}[ \nabla _{\theta } \mu _{\theta }( s) \nabla _{a} Q_{\mu _{\theta }}( s,a) |_{a=\mu _{\theta }( s)}]
\end{equation}
\begin{equation}
\approx \sum _{t=0}^{T-1} \nabla _{\theta } \mu _{\theta }( s_{t}) \nabla _{a} Q_{\mu _{\theta }}( s_{t} ,a_{t}) |_{a_{t} =\mu _{\theta }( s_{t})}
\end{equation}where \( \nabla _{\theta } \mu _{\theta }( s_{t})\) is \( M\times N\) Jacobian, \( M\) is action dimension and \( N\) is parameters spaces. Note also that the expectation is taken over state not action distribution. But deterministic, as the name suggested, do not benefit from sampling exploration, we migrate this by using off-policy method. The state distribution, \( p_{\beta }( s)\) is then according to behavior policy \( \beta \). 

Note however that in off-policy method where it often use importance sampling to compensate mismatch between generated and target policy. DPG doesn't have to since in stochastic policy, the gradient \( \nabla _{\theta } f\) have expectation over action but DPG does not, so it is negligible, we need only \( \nabla _{a} Q_{\mu _{\theta }}( s_{t} ,a_{t})\) but not \( \mathbb{E}_{a}[ \nabla Q_{\mu _{\theta }}( s_{t} ,a_{t}]\).

Similar to actor-critic framework, \( Q_{\mu _{\theta }}\) can be a function approximator, \( Q_{w}\) called a critic and actor \( \mu _{\theta }\) is updating by gradient ascent. We can use Sarsa update to estimate \( Q_{w}\) (alg: Bato and Sutton p.130). The off-policy can also use Q-learning as an updates for critic \( Q_{w}\).

DDPG is also class of deterministic policy, where they introduce noise for better exploration: \( \mu _{\theta }( s) +noise\). And using soft update on policy by forming convex combination of subsequent parameter updates, \( \theta ^{'}\leftarrow \theta ^{'} s+\theta ( 1-s)\). Remember that this combination is a mixture of some point in given parameters space and \( s\) is a line between it. This smooth an update between target \( \theta ^{'}\) and behavior policy \( \theta \). (code)
</p>
<h4>
Trust Region Policy Optimization [papers|code, code, code, code]  </h4>
<p>
The theoretical ideal to update the policy is maximizing expected reward while maintaining the stability of training. Which is they restrict the distributional distance of subsequent policy update, by using KL-divergence:
\begin{equation}
\mathbf{max}_{\theta }\mathbb{E}_{s\sim p( s) ,\ a\sim q( a)}\left[\frac{\pi _{\theta }( a|s)}{\pi _{\theta ^{old}}( a|s)} Q_{\theta ^{old}}( s,a)\right] \ \mathbf{subject\ to} \ \mathbb{E}_{s\sim p( s)}[ KL( \pi _{\theta ^{old}}( a|s) \| \pi _{\theta }( a|s)]
\end{equation}
The key is to make the biggest step while balancing the difference of policy update. As the name suggest, it first determine the search direction \( p\) follow by performing line search method. In the same sense as general trust region method, they approximate constraint as:
\begin{equation}
f_{0} =KL( \pi _{\theta ^{old}} \| \pi _{\theta }) \approx f_{0} +\nabla f_{0}^{T}( \theta -\theta _{k}) +\frac{1}{2}( \theta -\theta _{k})^{T} A( \theta -\theta _{k}) \leqslant \delta 
\end{equation}
and for objective function \( f\approx f+\nabla f^{T}( \theta -\theta _{k})\) where \( \nabla f^{T}\) is the same as policy gradient and \( A\) is hessian of KL-divergence with respect to parameters. Note that higher order expansion for \( f\) do not matter since it will mostly dominate by KL divergence term. Taking taylor expansion on KL divergence is having the terms of Fisher information \( A\). The algorithm is then 
\begin{equation}
\mathbf{max}_{\theta } \nabla f^{T}( \theta -\theta _{k}) \ \mathbf{subject\ to} \ \frac{1}{2}( \theta -\theta _{k})^{T} A( \theta -\theta _{k}) \leqslant \delta 
\end{equation}Solving this for \( \theta \) analytically using duality yield the langragian \( Z( \theta ,\lambda ) =\nabla f^{T}( \theta -\theta _{k}) +\lambda ( f_{0} -\delta )\) \( f_{0}\) is approximate KL constraint, it is quadratic in \( ( \theta -\theta _{k})\), so it has global minimum. Taking the derivative, set the langragian \( Z\) to zero and solve for \( \theta \): \( \nabla _{\theta } f-\lambda F_{\theta ^{old}} \nabla \theta =0\) and \( \nabla \theta =\frac{1}{\lambda } F^{-1} \nabla _{\theta } f\) which is the form of natural gradient. Full derivation can be found in here. We have update of the form
\begin{equation}
\theta _{k+1}\leftarrow \theta _{k} +\sqrt{\frac{2\delta }{x^{T} H^{-1} x}} H^{-1} g\ 
\end{equation}
In computing \( H^{-1} g\ \), instead of direct compute the inverse of hessian which is computational expensive and unstable, we use conjugate gradient to solve \( x=H^{-1} g\). This can be used as an update where the first term can be think like constant, it can be view as natural policy gradient, \( \theta _{k+1}^{natural}\)←\( \theta _{k+1}^{natural} +\alpha F^{-1} \nabla f\). But the taylor approximate introduce an error which may make this update do not satisfied the constraint. They then additionally use line search for finding best step \( \alpha \): \( \theta _{k+1}\leftarrow \theta _{k} +\alpha _{j}\sqrt{\frac{2\delta }{x^{T} H^{-1} x}} x\).
In computing hessian \( H\), one can directly compute it with reverse mode differentiation, other method such as BFGS is also proposed. But more efficient way is computing fisher vector product (\( Hx)\). Suppose that the function is neural network \( f_{n}\) and it map to distributional parameters that parametrized the distribution of action. The former is much easier to implement. 

TRPO is in class of MM algorithm where it optimize a lower bound (surrogate), approximate of \( f\) and therefore it never overestimate the value of the objective. The lower bound, the different of the objective of two policy is proof in appendix A in papers as \( f_{\pi ^{old}} -f_{\pi ^{new}} \geqslant L-C\sqrt{KL( \pi _{old} \| \pi )}\), where \( L\) is expectation of policy objective. 

PPO [code] is simplified version of TRPO aiming at the same goal, maximize the step while maintaining stability of policy update not going too far and crash training process. There exist two variant of PPO: KL penalty and clipped gradient. 
</p>
</div>

          </div>
        </div><!--/blog_post--><!--/post_details-->
      </div>
    </section>
    
    
    <footer class="u-clearfix u-footer u-grey-80" id="sec-e381"><div class="u-clearfix u-sheet u-sheet-1">
        <div class="u-align-left u-social-icons u-spacing-10 u-social-icons-1">
          <a class="u-social-url" title="facebook" target="_blank" href="https://www.facebook.com/owan.naruemit"><span class="u-icon u-social-facebook u-social-icon u-icon-1"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-9909"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-9909"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M73.5,31.6h-9.1c-1.4,0-3.6,0.8-3.6,3.9v8.5h12.6L72,58.3H60.8v40.8H43.9V58.3h-8V43.9h8v-9.2
            c0-6.7,3.1-17,17-17h12.5v13.9H73.5z"></path></svg></span>
          </a>
          <a class="u-social-url" title="twitter" target="_blank" href="https://twitter.com/napanaruemit"><span class="u-icon u-social-icon u-social-twitter u-icon-2"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-d07a"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-d07a"><circle fill="currentColor" class="st0" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M83.8,47.3c0,0.6,0,1.2,0,1.7c0,17.7-13.5,38.2-38.2,38.2C38,87.2,31,85,25,81.2c1,0.1,2.1,0.2,3.2,0.2
            c6.3,0,12.1-2.1,16.7-5.7c-5.9-0.1-10.8-4-12.5-9.3c0.8,0.2,1.7,0.2,2.5,0.2c1.2,0,2.4-0.2,3.5-0.5c-6.1-1.2-10.8-6.7-10.8-13.1
            c0-0.1,0-0.1,0-0.2c1.8,1,3.9,1.6,6.1,1.7c-3.6-2.4-6-6.5-6-11.2c0-2.5,0.7-4.8,1.8-6.7c6.6,8.1,16.5,13.5,27.6,14
            c-0.2-1-0.3-2-0.3-3.1c0-7.4,6-13.4,13.4-13.4c3.9,0,7.3,1.6,9.8,4.2c3.1-0.6,5.9-1.7,8.5-3.3c-1,3.1-3.1,5.8-5.9,7.4
            c2.7-0.3,5.3-1,7.7-2.1C88.7,43,86.4,45.4,83.8,47.3z"></path></svg></span>
          </a>
          <a class="u-social-url" title="linkedin" target="_blank" href="https://www.linkedin.com/in/pethai-napanaruemit-3a69a7190/"><span class="u-icon u-social-icon u-social-linkedin u-icon-3"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-694a"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-694a"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M41.3,83.7H27.9V43.4h13.4V83.7z M34.6,37.9L34.6,37.9c-4.6,0-7.5-3.1-7.5-7c0-4,3-7,7.6-7s7.4,3,7.5,7
            C42.2,34.8,39.2,37.9,34.6,37.9z M89.6,83.7H76.2V62.2c0-5.4-1.9-9.1-6.8-9.1c-3.7,0-5.9,2.5-6.9,4.9c-0.4,0.9-0.4,2.1-0.4,3.3v22.5
            H48.7c0,0,0.2-36.5,0-40.3h13.4v5.7c1.8-2.7,5-6.7,12.1-6.7c8.8,0,15.4,5.8,15.4,18.1V83.7z"></path></svg></span>
          </a>
          <a class="u-social-url" target="_blank" data-type="Github" title="Github" href="https://github.com/pethai2004"><span class="u-icon u-social-github u-social-icon u-icon-4"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-a39c"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-a39c"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M88,51.3c0-5.5-1.9-10.2-5.3-13.7c0.6-1.3,2.3-6.5-0.5-13.5c0,0-4.2-1.4-14,5.3c-4.1-1.1-8.4-1.7-12.7-1.8
	c-4.3,0-8.7,0.6-12.7,1.8c-9.7-6.6-14-5.3-14-5.3c-2.8,7-1,12.2-0.5,13.5C25,41.2,23,45.7,23,51.3c0,19.6,11.9,23.9,23.3,25.2
	c-1.5,1.3-2.8,3.5-3.2,6.8c-3,1.3-10.2,3.6-14.9-4.3c0,0-2.7-4.9-7.8-5.3c0,0-5-0.1-0.4,3.1c0,0,3.3,1.6,5.6,7.5c0,0,3,9.1,17.2,6
	c0,4.3,0.1,8.3,0.1,9.5h25.2c0-1.7,0.1-7.2,0.1-14c0-4.7-1.7-7.9-3.4-9.4C76,75.2,88,70.9,88,51.3z"></path></svg></span>
          </a>
          <a class="u-social-url" target="_blank" data-type="YouTube" title="YouTube" href="https://www.youtube.com/channel/UCkQZQFB-PPBgNEL-GizSucw"><span class="u-icon u-social-icon u-social-youtube u-icon-5"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-7985"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-7985"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M74.9,33.3H37.3c-7.4,0-13.4,6-13.4,13.4v18.8c0,7.4,6,13.4,13.4,13.4h37.6c7.4,0,13.4-6,13.4-13.4V46.7 C88.3,39.3,82.3,33.3,74.9,33.3L74.9,33.3z M65.9,57l-17.6,8.4c-0.5,0.2-1-0.1-1-0.6V47.5c0-0.5,0.6-0.9,1-0.6l17.6,8.9 C66.4,56,66.4,56.8,65.9,57L65.9,57z"></path></svg></span>
          </a>
        </div>
        <div class="u-border-1 u-border-white u-expanded-width u-line u-line-horizontal u-opacity u-opacity-50 u-line-1"></div>
      </div></footer>
    <section class="u-backlink u-clearfix u-grey-80">
      <a class="u-link" href="https://nicepage.com/website-templates" target="_blank">
        <span>Website Templates</span>
      </a>
      <p class="u-text">
        <span>created with</span>
      </p>
      <a class="u-link" href="" target="_blank">
        <span>Website Builder Software</span>
      </a>. 
    </section>
  
</body></html>
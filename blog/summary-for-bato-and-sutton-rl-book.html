<!doctype html>

  <script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<html style="font-size: 16px;" lang="en"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    <meta name="keywords" content="Post 1 Headline">
    <meta name="description" content="">
    <title></title>
    <link rel="stylesheet" href="../nicepage.css" media="screen">
<link rel="stylesheet" href="../Post-Template.css" media="screen">
    <script class="u-script" type="text/javascript" src="../jquery.js" defer=""></script>
    <script class="u-script" type="text/javascript" src="../nicepage.js" defer=""></script>
    <meta name="generator" content="Nicepage 4.14.1, nicepage.com">
    <link id="u-theme-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i|Open+Sans:300,300i,400,400i,500,500i,600,600i,700,700i,800,800i">
    
    
    <script type="application/ld+json">{
		"@context": "http://schema.org",
		"@type": "Organization",
		"name": "",
		"logo": "images/owanlogo.png",
		"sameAs": [
				"https://www.facebook.com/owan.naruemit",
				"https://twitter.com/napanaruemit",
				"https://www.linkedin.com/in/pethai-napanaruemit-3a69a7190/",
				"https://github.com/pethai2004",
				"https://www.youtube.com/channel/UCkQZQFB-PPBgNEL-GizSucw"
		]
}</script>
    <meta name="theme-color" content="#478ac9">
    <meta name="twitter:site" content="@">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Post Template">
    <meta name="twitter:description" content="">
  </head>
  <body class="u-body u-xl-mode" data-lang="en"><header class="u-clearfix u-header u-palette-1-light-2 u-header" id="sec-b8de"><div class="u-clearfix u-sheet u-sheet-1">
        <a href="../Home.html" data-page-id="208602995" class="u-image u-logo u-image-1" data-image-width="1000" data-image-height="1000" title="Home">
          <img src="../images/owanlogo.png" class="u-logo-image u-logo-image-1">
        </a>
        <nav class="u-menu u-menu-dropdown u-offcanvas u-menu-1">
          <div class="menu-collapse" style="font-size: 1rem; letter-spacing: 0px; font-weight: 700;">
            <a class="u-button-style u-custom-left-right-menu-spacing u-custom-padding-bottom u-custom-top-bottom-menu-spacing u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="#">
              <svg class="u-svg-link" viewBox="0 0 24 24"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#menu-hamburger"></use></svg>
              <svg class="u-svg-content" version="1.1" id="menu-hamburger" viewBox="0 0 16 16" x="0px" y="0px" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><g><rect y="1" width="16" height="2"></rect><rect y="7" width="16" height="2"></rect><rect y="13" width="16" height="2"></rect>
</g></svg>
            </a>
          </div>
          <div class="u-custom-menu u-nav-container">
            <ul class="u-nav u-unstyled u-nav-1"><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="../Home.html" style="padding: 10px 20px;">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="../project-and-code.html" style="padding: 10px 20px;">project and code</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="../lecture.html" style="padding: 10px 20px;">lecture</a>
</li></ul>
          </div>
          <div class="u-custom-menu u-nav-container-collapse">
            <div class="u-black u-container-style u-inner-container-layout u-opacity u-opacity-95 u-sidenav">
              <div class="u-inner-container-layout u-sidenav-overflow">
                <div class="u-menu-close"></div>
                <ul class="u-align-center u-nav u-popupmenu-items u-unstyled u-nav-2"><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../Home.html">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../project-and-code.html">project and code</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../lecture.html">lecture</a>
</li></ul>
              </div>
            </div>
            <div class="u-black u-menu-overlay u-opacity u-opacity-70"></div>
          </div>
        </nav>
      </div></header>
    <section class="u-align-center u-clearfix u-section-1" id="sec-bf54">
      <div class="u-clearfix u-sheet u-valign-middle-md u-valign-middle-sm u-valign-middle-xs u-sheet-1"><!--post_details--><!--post_details_options_json--><!--{"source":""}--><!--/post_details_options_json--><!--blog_post-->
        <div class="u-container-style u-expanded-width u-post-details u-post-details-1">
          <div class="u-container-layout u-valign-middle u-container-layout-1"><!--blog_post_image-->
            <!--/blog_post_image--><!--blog_post_header-->
            <h2 class="u-blog-control u-text u-text-1"></h2><!--/blog_post_header--><!--blog_post_metadata-->
            <div class="u-blog-control u-metadata u-metadata-1"><!--blog_post_metadata_date-->
              <!--/blog_post_metadata_date--><!--blog_post_metadata_category-->
              <!--/blog_post_metadata_category--><!--blog_post_metadata_comments-->
              <!--/blog_post_metadata_comments-->
            </div><!--/blog_post_metadata--><!--blog_post_content-->
            <div class="u-align-justify u-blog-control u-post-content u-text u-text-2 fr-view">
  
  </div><div style= "position: center;padding: 100px 100px;">

<h4>Chapter 2 Multi-armed Bandits</h4>
<p>
The true expected value of \( q_{*}( a) \equiv \mathbb{E}[ R_{t} |A_{t} =a]\), the ideal then is to choose action \( a\in A\) which maximize expected reward. In is obvious that such value is not known. One naive approach is estimate it \( q_{*}( a) =Q_{t}( a)\) is estimate at time \( t\): \( Q_{t} =\frac{\sum _{i=1}^{t-1} R_{i} \cdot \mathbb{1}_{A_{i} =a}}{\sum _{i=1}^{t-1}\mathbb{1}_{A_{i} =a}}\). As the number of sample go to infinity (the denorminator also approach inf), it converge to true estimate. The estimate require storing reward information, another incremental estimate is having the form:
\begin{equation*}
NewEstimate\leftarrow OldEstimate+StepSize\ *\ [ Target-OldEstimate]
\end{equation*}
We can write \( Q_{n+1}\) as \( Q_{n+1} =Q_{n} +1/n[ R_{n} -Q_{n}]\) (see page 31). When reward is nonstationary, it is desirable to weight more recent reward, it turns out that by using state-size, the estimate also being weighted at the same time (see p.32)\begin{equation}
Q_{n+1} =Q_{n} +\alpha [ R_{n} -Q_{n}] =( 1-\alpha )^{n} Q_{1} +\sum _{i=1}^{n} \alpha ( 1-\alpha )^{n-1} R_{i}
\end{equation}
Stochastic approximation theory condition that the step size is large enough:\( \sum _{n=1}^{\infty } a_{n} =\infty \) and eventually become small \) \)\( \sum _{n=1}^{\infty } a_{n}^{2} < \infty \). The update is bias in estimate of initial value \( Q_{1}\), if select action greedily and we overestimate the value of \( Q_{1}\), this is sometimes beneficial since the policy select optimistic one that not actually true, it thus explore more. This is non practical in general nonstationary problem. We can simply make update confidence bound on \( Q\) and selecting action by \( A_{t} =\mathbf{argmax}_{a}[ Q+cB]\) where \( B\) is some value, one formulation is where:
\begin{equation}
Q_{opt} =Q_{t}( a) +c\sqrt{\frac{\ln t}{N_{t}( a)}}
\end{equation}
The natural log means that the increase get smaller over time but unbounded.
</p>
<h4>
Chapter 3 Finite MDPs</h4>
<p>
RL in general is formularized as a finite MDPs of sequential decision making where action is influence not only current time \( t\) but also its subsequent. The dynamics of MDPs is defined as
\begin{equation}
p\left( s^{'} ,r|s,a\right) =Pr\left\{S_{t} =s^{'} ,R_{t} =r|S_{t-1} =s,\ A_{t-1} =a\right\}
\end{equation}
and the probability \( p\left( s^{'} ,r|s,a\right)\) sum up to one over state \( s\in S^{'}\)and \( r\in R\). In finite markov we assume that observation represent all knowledge of state, latter for approximation method we relax this condition and learn from non-Markov observations. Sum out reward we get state transition:
\begin{equation}
p\left( s^{'} |s,a\right) =\sum _{r\in R} p\left( s^{'} ,r|s,a\right)
\end{equation}
and expected reward is
\begin{equation}
\mathbb{E}[ R_{t} |S_{t-1} =s,A_{t-1} =a] =\mathbb{E}\left[\sum _{s^{'} \in S} p\left( s^{'} ,r|s,a\right)\right] =\sum _{r\in R} r\sum _{s^{'} \in S} p\left( s^{'} ,r|s,a\right)
\end{equation}
the last equation is
\begin{equation}
\mathbb{E}\left[ R_{t} |S_{t-1} =s,A_{t-1} =a,S_{t} =s^{'}\right] =\sum _{r\in R} r\frac{p\left( s^{'} ,r|s,a\right)}{p\left( s^{'} |s,a\right)}
\end{equation}
Note that we can write \( G_{t} =R_{t+1} +\gamma G_{t+1}\) where \( G_{t}\) is discounted returns. For example if every time step the reward is constant one, the return is \( G_{t} =1/( 1-\gamma )\), it is geometric sequences. We estimate value function \( v_{\pi }( s)\) from generated by policy \( \pi \), is mesure how good at current state is since it is defined as expected discounted reward \( v_{\pi }( s) =\mathbb{E}_{\pi }[ G_{t} |S_{t} =s]\) and also action value function: \( q_{\pi }( s,a) =\mathbb{E}_{\pi }[ G_{t} |S_{t} =s,A_{t} =a]\). Since it is estimate as empirical return, we call it is Monte Carlo methods for estimating value function. The value function \( v_{\pi }( s)\) statisfies recursize relationship, it can be written as:
\begin{equation}
v_{\pi }( s) =\sum _{a} \pi ( a|s)\sum _{s^{'} ,r} p\left( s^{'} ,r|s,a\right)\left[ r+\gamma v_{\pi }\left( s^{'}\right)\right]
\end{equation}
It involve succesor \( v_{\pi }\left( s^{'}\right)\) of current value function \( v_{\pi }( s)\), thus is called Bellman equation for \( v_{\pi }( s)\). The optimal value is the value which the policy maximize the value: \( v_{*}( s) =\mathbf{max}_{\pi } v_{\pi }( s)\), the same go for action value. The action-value also can be written in term of value as:
\begin{equation}
q_{*}( s,a) =\mathbb{E}[ R_{t+1} +\gamma v_{*}( S_{t+1}) |S_{t} =s,A_{t} =a]
\end{equation}
</p>
<h4>
Chapter 4 Dynamic Programming</h4>
<p>
Suppose thorough this chapter that the MDPs is finite.
Policy Evaluation : Given policy \( \pi \) (not necessary optimal), what is its corresponding value function?
As I supposed, the dynamic of state distribution is stationary and known, the corresponding value \( v_{\pi }( s)\) is a system of \( |S|\) simultaneous equation (the number of possible state). The next update of value of state is (try to approximate \( v_{\pi }\))
\begin{equation}
v_{k+1}( s) =\sum _{a} \pi ( a|s)\sum _{s^{'} ,r} p\left( s^{'} ,r|s,a\right)\left[ r+\gamma v_{k}\left( s^{'}\right)\right]
\end{equation}
As \( k\) go to infinity, this approximate converge to true value of its policy \( v_{\pi }( s)\). Since in DP, the update in value function is finite and consider all possible next state and action not its sampled state.
Policy improvement : Having the value \( v_{\pi }\), how can we find better policy \( \pi \)?
Consider selecting action \( a\) in state \( s\) and follow current policy \( \pi \). The inequality \( q_{\pi }\left( s,\pi ^{/}( s)\right)  >v_{\pi }( s)\) holds if the changed in policy at one state before follow the same policy, is better than follow the same policy. This is policy improvement theorem : if we obtain a better policy by follow action \( a=\pi ^{'}( s)\) then the expected value if better, \( v_{\pi ^{'}} \geqslant v_{\pi }\). (proof in page 78) In general, if we follow the greedy policy:
\begin{gather*}
\pi ^{'}( s) =\mathrm{argmax}_{a} \ q_{\pi }( s,a)\\
=\mathrm{argmax}_{a}\sum _{s^{'} ,r} p\left( s^{'} ,r|s,a\right)\left[ r+\gamma v_{\pi }\left( s^{'}\right)\right]
\end{gather*}
where \( v_{\pi }\left( s^{'}\right)\) is one-step look ahead for next value of next state. This is policy improvement, finding new policy by follow the greedy of value function of original policy. 
Policy iteration : A iterative computation of policy evaluation and improvement.
This we obtain the sequences of approximate in finite MDPs, \( \pi _{0}\stackrel{eval}{\rightarrow } v_{\pi _{0}}\stackrel{imp}{\rightarrow } \pi _{0} \ \cdotp \cdotp \cdotp \rightarrow v_{*} \ ,\ \pi _{*} \ \). It is also possible to truncate the policy evaluation or \( v_{\pi }\) and be not affect the improvement of both value. This is Value iteration where the value is combined the policy improvement and evaluation steps: 
\begin{equation*}
\mathrm{v_{k+1}( s) =max}_{a}\sum _{s^{'} ,r} p\left( s^{'} ,r|s,a\right)\left[ r+\gamma v_{k}\left( s^{'}\right)\right]
\end{equation*}This is update that turn Bellman equation to update rule and is identical to (9) but the action is deterministic with respect to its maximum. (see algorithm page 83)
</p>
<h4>
Chapter 5  Monte Carlo Methods</h4>
<p>
This is a more approach to practical solution to general problem where there are infinitely possible state and action spaces in which knowledge of the system is (generally) not known. We acquire ,instead of state transition probability, we have sample transition. And is in the sence that MC is episode-by-episode method (not online step-by-step). 

exploration start : Assigning nonzero probability to every action in the first state ensure exploration ensure that the all state action pairs are visited. In general stochestic action is more useful.
</p>
</div>

          </div>
        </div><!--/blog_post--><!--/post_details-->
      </div>
    </section>
    
    
    <footer class="u-clearfix u-footer u-grey-80" id="sec-e381"><div class="u-clearfix u-sheet u-sheet-1">
        <div class="u-align-left u-social-icons u-spacing-10 u-social-icons-1">
          <a class="u-social-url" title="facebook" target="_blank" href="https://www.facebook.com/owan.naruemit"><span class="u-icon u-social-facebook u-social-icon u-icon-1"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-9909"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-9909"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M73.5,31.6h-9.1c-1.4,0-3.6,0.8-3.6,3.9v8.5h12.6L72,58.3H60.8v40.8H43.9V58.3h-8V43.9h8v-9.2
            c0-6.7,3.1-17,17-17h12.5v13.9H73.5z"></path></svg></span>
          </a>
          <a class="u-social-url" title="twitter" target="_blank" href="https://twitter.com/napanaruemit"><span class="u-icon u-social-icon u-social-twitter u-icon-2"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-d07a"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-d07a"><circle fill="currentColor" class="st0" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M83.8,47.3c0,0.6,0,1.2,0,1.7c0,17.7-13.5,38.2-38.2,38.2C38,87.2,31,85,25,81.2c1,0.1,2.1,0.2,3.2,0.2
            c6.3,0,12.1-2.1,16.7-5.7c-5.9-0.1-10.8-4-12.5-9.3c0.8,0.2,1.7,0.2,2.5,0.2c1.2,0,2.4-0.2,3.5-0.5c-6.1-1.2-10.8-6.7-10.8-13.1
            c0-0.1,0-0.1,0-0.2c1.8,1,3.9,1.6,6.1,1.7c-3.6-2.4-6-6.5-6-11.2c0-2.5,0.7-4.8,1.8-6.7c6.6,8.1,16.5,13.5,27.6,14
            c-0.2-1-0.3-2-0.3-3.1c0-7.4,6-13.4,13.4-13.4c3.9,0,7.3,1.6,9.8,4.2c3.1-0.6,5.9-1.7,8.5-3.3c-1,3.1-3.1,5.8-5.9,7.4
            c2.7-0.3,5.3-1,7.7-2.1C88.7,43,86.4,45.4,83.8,47.3z"></path></svg></span>
          </a>
          <a class="u-social-url" title="linkedin" target="_blank" href="https://www.linkedin.com/in/pethai-napanaruemit-3a69a7190/"><span class="u-icon u-social-icon u-social-linkedin u-icon-3"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-694a"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-694a"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M41.3,83.7H27.9V43.4h13.4V83.7z M34.6,37.9L34.6,37.9c-4.6,0-7.5-3.1-7.5-7c0-4,3-7,7.6-7s7.4,3,7.5,7
            C42.2,34.8,39.2,37.9,34.6,37.9z M89.6,83.7H76.2V62.2c0-5.4-1.9-9.1-6.8-9.1c-3.7,0-5.9,2.5-6.9,4.9c-0.4,0.9-0.4,2.1-0.4,3.3v22.5
            H48.7c0,0,0.2-36.5,0-40.3h13.4v5.7c1.8-2.7,5-6.7,12.1-6.7c8.8,0,15.4,5.8,15.4,18.1V83.7z"></path></svg></span>
          </a>
          <a class="u-social-url" target="_blank" data-type="Github" title="Github" href="https://github.com/pethai2004"><span class="u-icon u-social-github u-social-icon u-icon-4"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-a39c"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-a39c"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M88,51.3c0-5.5-1.9-10.2-5.3-13.7c0.6-1.3,2.3-6.5-0.5-13.5c0,0-4.2-1.4-14,5.3c-4.1-1.1-8.4-1.7-12.7-1.8
	c-4.3,0-8.7,0.6-12.7,1.8c-9.7-6.6-14-5.3-14-5.3c-2.8,7-1,12.2-0.5,13.5C25,41.2,23,45.7,23,51.3c0,19.6,11.9,23.9,23.3,25.2
	c-1.5,1.3-2.8,3.5-3.2,6.8c-3,1.3-10.2,3.6-14.9-4.3c0,0-2.7-4.9-7.8-5.3c0,0-5-0.1-0.4,3.1c0,0,3.3,1.6,5.6,7.5c0,0,3,9.1,17.2,6
	c0,4.3,0.1,8.3,0.1,9.5h25.2c0-1.7,0.1-7.2,0.1-14c0-4.7-1.7-7.9-3.4-9.4C76,75.2,88,70.9,88,51.3z"></path></svg></span>
          </a>
          <a class="u-social-url" target="_blank" data-type="YouTube" title="YouTube" href="https://www.youtube.com/channel/UCkQZQFB-PPBgNEL-GizSucw"><span class="u-icon u-social-icon u-social-youtube u-icon-5"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-7985"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-7985"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M74.9,33.3H37.3c-7.4,0-13.4,6-13.4,13.4v18.8c0,7.4,6,13.4,13.4,13.4h37.6c7.4,0,13.4-6,13.4-13.4V46.7 C88.3,39.3,82.3,33.3,74.9,33.3L74.9,33.3z M65.9,57l-17.6,8.4c-0.5,0.2-1-0.1-1-0.6V47.5c0-0.5,0.6-0.9,1-0.6l17.6,8.9 C66.4,56,66.4,56.8,65.9,57L65.9,57z"></path></svg></span>
          </a>
        </div>
        <div class="u-border-1 u-border-white u-expanded-width u-line u-line-horizontal u-opacity u-opacity-50 u-line-1"></div>
      </div></footer>
    <section class="u-backlink u-clearfix u-grey-80">
      <a class="u-link" href="https://nicepage.com/website-templates" target="_blank">
        <span>Website Templates</span>
      </a>
      <p class="u-text">
        <span>created with</span>
      </p>
      <a class="u-link" href="" target="_blank">
        <span>Website Builder Software</span>
      </a>. 
    </section>
  
</body></html>
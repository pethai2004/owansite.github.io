<!doctype html>
  <script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<html style="font-size: 16px;" lang="en"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    <meta name="keywords" content="Post 1 Headline">
    <meta name="description" content="">
    <title>Regression problem</title>
    <link rel="stylesheet" href="../nicepage.css" media="screen">
<link rel="stylesheet" href="../Post-Template.css" media="screen">
    <script class="u-script" type="text/javascript" src="../jquery.js" defer=""></script>
    <script class="u-script" type="text/javascript" src="../nicepage.js" defer=""></script>
    <meta name="generator" content="Nicepage 4.14.1, nicepage.com">
    <link id="u-theme-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i|Open+Sans:300,300i,400,400i,500,500i,600,600i,700,700i,800,800i">
    
    
    <script type="application/ld+json">{
		"@context": "http://schema.org",
		"@type": "Organization",
		"name": "",
		"logo": "images/owanlogo.png",
		"sameAs": [
				"https://www.facebook.com/owan.naruemit",
				"https://twitter.com/napanaruemit",
				"https://www.linkedin.com/in/pethai-napanaruemit-3a69a7190/",
				"https://github.com/pethai2004",
				"https://www.youtube.com/channel/UCkQZQFB-PPBgNEL-GizSucw"
		]
}</script>
    <meta name="theme-color" content="#478ac9">
    <meta name="twitter:site" content="@">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Post Template">
    <meta name="twitter:description" content="">
  </head>
  <body class="u-body u-xl-mode" data-lang="en"><header class=" u-clearfix u-header u-section-row-container" id="sec-b8de"><div class="u-section-rows">
        <div class="u-clearfix u-palette-2-light-1 u-section-row u-section-row-1" data-animation-name="" data-animation-duration="0" data-animation-delay="0" data-animation-direction="" id="sec-f7ac">
          <div class="u-clearfix u-sheet u-valign-middle-lg u-valign-middle-md u-valign-middle-sm u-valign-middle-xl u-sheet-1">
            <a href="../Home.html" data-page-id="208602995" class="u-image u-logo u-image-1" data-image-width="1000" data-image-height="1000" title="Home">
              <img src="../images/owanlogo.png" class="u-logo-image u-logo-image-1">
            </a>
            <nav class="u-menu u-menu-dropdown u-offcanvas u-menu-1">
              <div class="menu-collapse" style="font-size: 1rem; letter-spacing: 0px; font-weight: 700;">
                <a class="u-button-style u-custom-left-right-menu-spacing u-custom-padding-bottom u-custom-top-bottom-menu-spacing u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="#" style="padding: 4px 42px; font-size: calc(1em + 8px);">
                  <svg class="u-svg-link" viewBox="0 0 24 24"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#menu-hamburger"></use></svg>
                  <svg class="u-svg-content" version="1.1" id="menu-hamburger" viewBox="0 0 16 16" x="0px" y="0px" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><g><rect y="1" width="16" height="2"></rect><rect y="7" width="16" height="2"></rect><rect y="13" width="16" height="2"></rect>
</g></svg>
                </a>
              </div>
              <div class="u-custom-menu u-nav-container">
                <ul class="u-nav u-unstyled u-nav-1"><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="../Home.html" style="padding: 10px 20px;">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="../project-and-code.html" style="padding: 10px 20px;">project and code</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="../lecture.html" style="padding: 10px 5px 10px 20px;">lecture</a>
</li></ul>
              </div>
              <div class="u-custom-menu u-nav-container-collapse">
                <div class="u-black u-container-style u-inner-container-layout u-opacity u-opacity-95 u-sidenav">
                  <div class="u-inner-container-layout u-sidenav-overflow">
                    <div class="u-menu-close"></div>
                    <ul class="u-align-center u-nav u-popupmenu-items u-unstyled u-nav-2"><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../Home.html">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../project-and-code.html">project and code</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../lecture.html">lecture</a>
</li></ul>
                  </div>
                </div>
                <div class="u-black u-menu-overlay u-opacity u-opacity-70"></div>
              </div>
            </nav>
          </div>
          
          
          
          
          
        </div>
        <div class="u-section-row u-section-row-2" id="sec-e839">
          <div class="u-clearfix u-sheet u-sheet-2"></div>
          
          
          
          
          
        </div>
      </div></header>
    <section class="u-align-center u-clearfix u-section-1" id="sec-bf54">
      <div class="u-clearfix u-sheet u-valign-middle-md u-valign-middle-sm u-valign-middle-xs u-sheet-1"><!--post_details--><!--post_details_options_json--><!--{"source":""}--><!--/post_details_options_json--><!--blog_post-->
        <div class="u-container-style u-expanded-width u-post-details u-post-details-1">
          <div class="u-container-layout u-valign-middle u-container-layout-1"><!--blog_post_image-->
            <!--/blog_post_image--><!--blog_post_header-->
            <h2 class="u-blog-control u-text u-text-1">Regression problem</h2><!--/blog_post_header--><!--blog_post_metadata-->
            <div class="u-blog-control u-metadata u-metadata-1"><!--blog_post_metadata_date-->
              <span class="u-meta-date u-meta-icon">Aug 13, 2022</span><!--/blog_post_metadata_date--><!--blog_post_metadata_category-->
              <!--/blog_post_metadata_category--><!--blog_post_metadata_comments-->
              <!--/blog_post_metadata_comments-->
            </div><!--/blog_post_metadata--><!--blog_post_content-->
            <div class="u-align-justify u-blog-control u-post-content u-text u-text-2 fr-view">
  
  </div><div style= "position: center;padding: 100px 100px;">

<h4>Linear Regression</h4>
<p>
  1. (MLE) can be applied here by maximizing likelihood function typically via gradient 
descent for negative-log likelihood. Note that we, in practice, minimize log-transformation of likelihood. This also give advantage for computation of gradient; gradient of the objective is sum of individual gradients of N terms. However, closed-form solution does exist and obtain by compute derivative of objective with respect to parameters
\begin{equation}
\frac{d}{d\theta } L( \theta ) =-\frac{d}{d\theta }\log\mathcal{N}( x|\theta ) =\frac{1}{2\sigma ^{2}}\frac{d}{d\theta } \| y-X\theta \| ^{2} +\delta 
\end{equation}
Here \( \delta \) is sum constant and can be ignored. Since the objective is convex, we obtain unique global optimum by setting to zero and solve for \( \theta \). Full derivation can be found in math for ML. We can also transform feature vector into nonlinearity since linear regression refer the “linear” to only parameters. By applied some function \( \psi ( x)\), mapping from \( \psi :\mathbb{R}^{D}\rightarrow \mathbb{R}\) for each vector. One such example is polynomial regression given by \( f( x) =\psi ^{T}( x) \theta =\sum _{i=1}^{K} \theta _{i} x^{( i)}\) where \( \theta \in \mathbb{R}^{K}\). We then can define feature matrix as\begin{equation}
\Psi =\left(\begin{matrix}
\psi ^{T}( x_{1})\\
\vdots \\
\psi ^{T}( x_{2})
\end{matrix}\right) =\left(\begin{matrix}
\psi _{0} (x_{1} ) & \cdots  & \psi _{K-1} (x_{1} )\\
\vdots  & \ddots  & \vdots \\
\psi _{0} (x_{N} ) & \cdots  & \psi _{K} (x_{N} )
\end{matrix}\right)
\end{equation}
where \( \Psi \in \mathbb{R}^{N\times K}\) and \( \psi _{j} :\mathbb{R}^{D}\rightarrow \mathbb{R}\). Some technical requirements of feature can be found in math for ML (p.297). We can search of best degree of polynomial M that minimize RMSE (root mean square error) for sufficient test between 0 and amount of data. If number of parameters is greater than data point, it would be infinitely possible maximum likelihood estimator. Note that I assume Noise variance \( \sigma ^{2}\) to be known. But we can also follow the same principle and obtain estimator of variance by (MLE). 

  2. (MAP) motivated by the fact that using MLE is prone to huge parameters magnitude and 
overfitting, we can place prior knowledge on parameters. For example, by using gaussian prior, we expect that the parameters lie withing some interval. Following the similar procedure as MLE, we by maximize posterior 
\begin{equation}
\hat{\theta } =\mathbf{argmax}_{\theta }\log p( \theta |X,Y) =\mathbf{argmax}_{\theta }\log p( Y|X,\theta ) ,\ \log p( \theta ) +\delta 
\end{equation}
Then compute the gradient of loss then set it to zero. We then arrive at MAP estimate
\begin{equation}
\hat{\theta } =\left( \Psi ^{T} \Psi +\frac{\sigma ^{2}}{b^{2}}\mathbb{I}\right)^{-1} \Psi ^{T} y
\end{equation}
The term \( \frac{\sigma ^{2}}{b^{2}}\mathbb{I}\) ensure \( \Psi ^{T} \Psi +\frac{\sigma ^{2}}{b^{2}}\mathbb{I}\) is symmetric and strictly positive definite (its inverse exist and has unique solution to linear equation). (see p. 302, math for ML). Note that \( b^{2}\) here is scalar variance of isotropic gaussian \( p( \theta ) =\mathcal{N}\left( 0|b^{2}\mathbb{I}\right)\). *Note (isotopic): consider computation of covariance is grow exponentially in size while variance only scale linearly, it’s often to represent covariance as \( \Sigma =b^{2}\mathbb{I}\).

We can use regularization with \( \| y-\Psi \theta \| +\gamma \| \theta \| ^{2}\), with second term (regularizer) can be interpret as negative log-gaussian prior that we use MAP as an estimator. By choosing Gaussian prior \( \mathcal{N}\left( 0|b^{2}\mathbb{I}\right)\), we obtain \( -\log p( \theta ) =\delta +\| \theta \| _{\epsilon }^{2} /2b^{2}\) where \( \gamma =1/2b^{2}\). In other word, log-gaussian prior and regularization term are identical. We see that minimizing regularized loss is closely related to minimizing negative log-likelihood plus prior, and we can obtain solution that is also identical \( \hat{\theta } =\left( \Psi ^{T} \Psi +\gamma \mathbb{I}\right)^{-1} \Psi ^{T} y\) where \( \gamma =\sigma ^{2} /b^{2}\)
</p>

<h4>Bayesian Linear Regression</h4>
<p>In Bayesian approach, we do not fit the parameters with single point estimate but use full posterior of parameters, by compute a mean over all plausible settings. We then usually do not interest in predicted value of parameters but what prediction it made. We integrate out the parameters and obtain expectation of a prediction over parameters distribution and can be interpreted as average.
The procedure is as follow:
  Choose prior prediction (typically Gaussian)
  Compute posterior over parameters using Bayes’ Theorem.
Here we choose conjugate prior \( p( \theta ) =\mathcal{N}( m,s)\) (mean and std as params), since its posterior (predictive distribution) is also Gaussian and have a closed form. Other prior may not have analytical posterior. Given the likelihood \( \mathcal{N}\left( y|\psi ( x) \theta ,\sigma ^{2}\right)\),  we arrive prediction distribution as \( p\left( f\left( x^{*}\right)\right) =\mathcal{N}\left( \psi \left( x^{*}\right) m,\psi ^{T}\left( x^{*}\right) s\psi \left( x^{*}\right)\right)\)
Here the \( p( f( \cdot ))\) is represented the distribution over function that have functional form of \( f( \cdot ) =\theta _{i}^{T} \psi ( \cdot )\). After choosing Gaussian as a prior we can explicitly obtain posterior of the parameters given training data. Often we do not interest in predictive distribution of some observation
</p>
<h4>Sampling Methods</h4>
<p>
The central interest in among machine learning to approximate kl-divergence in policy gradient method is an ideal to compute intractable form of expectation \( \mathbb{E}_{x\sim p} [f(x)]=f(x)p(x)dx\). This could be approximate by finite sum of sampled \( f:\hat{f} =\frac{1}{N}\sum _{t=1}^{N} f(x_{i} )\), where \( x\) is sampled from p which is drawn independently. Note on variance of the estimator \hat{f} does not depend on dimensionality of sampled data, high accuracy may achievable with less number of sampled data.

  Rejection sampling
This allow us to sample a complex distribution subject to some constraints. Consider proposal distribution \( q( x)\) and constant \( k\), we choose \( k\) such that \( k\) for all \( x\), \( kq( x)  >\hat{p}( x)\), where \( \hat{p}\) is unnormalized probability.
The process is simple: first we sample a point \( a_{0} \sim \ q( x)\), then generate number from uniform distribution \( c_{0} \sim \ \mathcal{U}\{0,\ kq(x)\}\),\ second if \( c_{0}  >\ \hat{p} (a_{0} )\) then the sample is rejected otherwise retained.  

Intuitively, only the value of \( c_{0}\) lies withing area under the curve of \( kq( x)\) and \( \hat{p}( x)\), otherwise samples are ignored. It can be easily see that the ratio of shaded are and are under \( \hat{p}( x)\) is the ratio of accepting ratio. Thus the probability of accepted sample is \( \int \frac{\hat{p} (x)\ }{kq(x)} dx=\frac{1}{k}\int \hat{p} (x)dx\). Now we may not want the sample to be rejected all the time, we see that constant k should be chosen as small as possible.
<p>
<center><img src="image/rj.png" alt="rejection sampling", style="width:300px;height:150px;"></center>
</p>
  Importance sampling
Consider when we wish to sample a distribution \( p( x\)) but it is hard to access this distribution. This arise in a lot in machine learning, for example, sampling from state distribution that is generated by different policy function in reinforcement learning. Suppose that evaluate \( p( x\)) for any input data can be easily done except sample from it. We instead sample from proposal distribution q(x) like Rejection sampling only we do not reject the samples. We rewrite the expectation: 
\begin{gather}
\mathbb{E}_{x\sim p}[ f( x)] =\int f( x) p( x) dx=\\
\int f( x)\frac{p( x)}{q( x)} q( x) dx\mathbb{E}_{x\sim q}\left[ f( x)\frac{p( x)}{q( x)}\right] \approx \frac{1}{N}\sum _{t=1}^{N} f(x_{i} )\frac{p(x_{i} )}{q(x_{i} )}
\end{gather}
Consider more of the sampling from unnormalized version of \( p( x)\) and \( q( x)\), we have
\begin{equation}
\mathbb{E}_{x\sim p}[ f( x)] \approx \frac{Z_{q}}{Z_{p}}\frac{1}{N}\sum _{t=1}^{N} f(x_{i} )\frac{\hat{p} (x_{i} )}{\hat{q} (x_{i} )}
\end{equation}
Where \( Z_{p}\) and \( Z_{q}\) are normalizing constant of \( \hat{p}( x)\) and \( \hat{q}( x)\) respectively. The key to sampling \( \hat{q}( x)\) should not be small region where \( \hat{p}( x)\)) is large. This follow the reason that we expect proposal distribution to match true distribution as possible and that \( f( x) p( x)\) often have its mass concentrated on small regions and sample lies withing that spaces.

  EM algorithm
Recall the procedure in E-step of EM algorithm is to compute the expectation given the observed data x and parameters \( \theta :Q(\theta ,\ \theta _{0} )=\mathbb{E} [ln(\theta ;\ X,\ YX,\theta _{0} )]\), where \( ln(\theta ;\ X,\ Y)=ln\ p(Y,\ X| \theta )\) is the complete-data log likelihood. Suppose on X is observable and Y is latent. This can use sampling to approximate the expectation: 
\begin{equation}
\mathbb{E} [ln( \theta ;X,Y) X,\theta _{0} )]=lnp(Y,X\theta )p(YX,\theta _{0} )dY\approx \frac{1}{N}\sum _{t=1}^{N} ln\ p(Y^{t} ,X| \theta )
\end{equation}
Then we follow the same procedure of M – step by maximizing the parameters such that \( \theta =\max_{\theta } Q( \theta ,\theta _{0})\)
  Data augmentation algorithm (generative model)
Also involving approximate the integral given before. The procedure involve twos step like EM consist of I and P steps. The ideal is to sample from distribution \( p( Y|X)\) however we instead drawn from indirect distribution involving the parameters \( \theta :p( Y|X,\theta )\) which drawn from current estimate of the posterior \( p( \theta |X)\). Then use it to sample the latent \( Y^{t}\) from : \( p( Y|X,\ \theta )\). This is done in I-step of the algorithm. In P-step, we use samples \( \left\{Y^{t}\right\}\) to compute a new estimate of the posterior over parameters \( \theta \) by

\begin{equation*}
p( \theta |X) =\int p( \theta |X,Y) p( Y|X) dY=\frac{1}{N}\sum _{t=1}^{N} p\left( \theta |X,Y^{t}\right)
\end{equation*}
</p>
</div>
          </div>
        </div><!--/blog_post--><!--/post_details-->
      </div>
    </section>
    
    
    <footer class="u-clearfix u-footer u-grey-80" id="sec-e381"><div class="u-clearfix u-sheet u-sheet-1">
        <div class="u-align-left u-social-icons u-spacing-10 u-social-icons-1">
          <a class="u-social-url" title="facebook" target="_blank" href="https://www.facebook.com/owan.naruemit"><span class="u-icon u-social-facebook u-social-icon u-icon-1"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-9909"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-9909"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M73.5,31.6h-9.1c-1.4,0-3.6,0.8-3.6,3.9v8.5h12.6L72,58.3H60.8v40.8H43.9V58.3h-8V43.9h8v-9.2
            c0-6.7,3.1-17,17-17h12.5v13.9H73.5z"></path></svg></span>
          </a>
          <a class="u-social-url" title="twitter" target="_blank" href="https://twitter.com/napanaruemit"><span class="u-icon u-social-icon u-social-twitter u-icon-2"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-d07a"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-d07a"><circle fill="currentColor" class="st0" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M83.8,47.3c0,0.6,0,1.2,0,1.7c0,17.7-13.5,38.2-38.2,38.2C38,87.2,31,85,25,81.2c1,0.1,2.1,0.2,3.2,0.2
            c6.3,0,12.1-2.1,16.7-5.7c-5.9-0.1-10.8-4-12.5-9.3c0.8,0.2,1.7,0.2,2.5,0.2c1.2,0,2.4-0.2,3.5-0.5c-6.1-1.2-10.8-6.7-10.8-13.1
            c0-0.1,0-0.1,0-0.2c1.8,1,3.9,1.6,6.1,1.7c-3.6-2.4-6-6.5-6-11.2c0-2.5,0.7-4.8,1.8-6.7c6.6,8.1,16.5,13.5,27.6,14
            c-0.2-1-0.3-2-0.3-3.1c0-7.4,6-13.4,13.4-13.4c3.9,0,7.3,1.6,9.8,4.2c3.1-0.6,5.9-1.7,8.5-3.3c-1,3.1-3.1,5.8-5.9,7.4
            c2.7-0.3,5.3-1,7.7-2.1C88.7,43,86.4,45.4,83.8,47.3z"></path></svg></span>
          </a>
          <a class="u-social-url" title="linkedin" target="_blank" href="https://www.linkedin.com/in/pethai-napanaruemit-3a69a7190/"><span class="u-icon u-social-icon u-social-linkedin u-icon-3"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-694a"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-694a"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M41.3,83.7H27.9V43.4h13.4V83.7z M34.6,37.9L34.6,37.9c-4.6,0-7.5-3.1-7.5-7c0-4,3-7,7.6-7s7.4,3,7.5,7
            C42.2,34.8,39.2,37.9,34.6,37.9z M89.6,83.7H76.2V62.2c0-5.4-1.9-9.1-6.8-9.1c-3.7,0-5.9,2.5-6.9,4.9c-0.4,0.9-0.4,2.1-0.4,3.3v22.5
            H48.7c0,0,0.2-36.5,0-40.3h13.4v5.7c1.8-2.7,5-6.7,12.1-6.7c8.8,0,15.4,5.8,15.4,18.1V83.7z"></path></svg></span>
          </a>
          <a class="u-social-url" target="_blank" data-type="Github" title="Github" href="https://github.com/pethai2004"><span class="u-icon u-social-github u-social-icon u-icon-4"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-a39c"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-a39c"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M88,51.3c0-5.5-1.9-10.2-5.3-13.7c0.6-1.3,2.3-6.5-0.5-13.5c0,0-4.2-1.4-14,5.3c-4.1-1.1-8.4-1.7-12.7-1.8
	c-4.3,0-8.7,0.6-12.7,1.8c-9.7-6.6-14-5.3-14-5.3c-2.8,7-1,12.2-0.5,13.5C25,41.2,23,45.7,23,51.3c0,19.6,11.9,23.9,23.3,25.2
	c-1.5,1.3-2.8,3.5-3.2,6.8c-3,1.3-10.2,3.6-14.9-4.3c0,0-2.7-4.9-7.8-5.3c0,0-5-0.1-0.4,3.1c0,0,3.3,1.6,5.6,7.5c0,0,3,9.1,17.2,6
	c0,4.3,0.1,8.3,0.1,9.5h25.2c0-1.7,0.1-7.2,0.1-14c0-4.7-1.7-7.9-3.4-9.4C76,75.2,88,70.9,88,51.3z"></path></svg></span>
          </a>
          <a class="u-social-url" target="_blank" data-type="YouTube" title="YouTube" href="https://www.youtube.com/channel/UCkQZQFB-PPBgNEL-GizSucw"><span class="u-icon u-social-icon u-social-youtube u-icon-5"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-7985"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-7985"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M74.9,33.3H37.3c-7.4,0-13.4,6-13.4,13.4v18.8c0,7.4,6,13.4,13.4,13.4h37.6c7.4,0,13.4-6,13.4-13.4V46.7 C88.3,39.3,82.3,33.3,74.9,33.3L74.9,33.3z M65.9,57l-17.6,8.4c-0.5,0.2-1-0.1-1-0.6V47.5c0-0.5,0.6-0.9,1-0.6l17.6,8.9 C66.4,56,66.4,56.8,65.9,57L65.9,57z"></path></svg></span>
          </a>
        </div>
        <div class="u-border-1 u-border-white u-expanded-width u-line u-line-horizontal u-opacity u-opacity-50 u-line-1"></div>
      </div></footer>
    <section class="u-backlink u-clearfix u-grey-80">
      <a class="u-link" href="https://nicepage.com/website-templates" target="_blank">
        <span>Website Templates</span>
      </a>
      <p class="u-text">
        <span>created with</span>
      </p>
      <a class="u-link" href="" target="_blank">
        <span>Website Builder Software</span>
      </a>. 
    </section>
  
</body></html>
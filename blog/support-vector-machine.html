<!doctype html>
  <script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<html style="font-size: 16px;" lang="en"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    <meta name="keywords" content="Post 1 Headline">
    <meta name="description" content="">
    <title>Support Vector Machine</title>
    <link rel="stylesheet" href="../nicepage.css" media="screen">
<link rel="stylesheet" href="../Post-Template.css" media="screen">
    <script class="u-script" type="text/javascript" src="../jquery.js" defer=""></script>
    <script class="u-script" type="text/javascript" src="../nicepage.js" defer=""></script>
    <meta name="generator" content="Nicepage 4.14.1, nicepage.com">
    <link id="u-theme-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i|Open+Sans:300,300i,400,400i,500,500i,600,600i,700,700i,800,800i">
    
    
    <script type="application/ld+json">{
		"@context": "http://schema.org",
		"@type": "Organization",
		"name": "",
		"logo": "images/owanlogo.png",
		"sameAs": [
				"https://www.facebook.com/owan.naruemit",
				"https://twitter.com/napanaruemit",
				"https://www.linkedin.com/in/pethai-napanaruemit-3a69a7190/",
				"https://github.com/pethai2004",
				"https://www.youtube.com/channel/UCkQZQFB-PPBgNEL-GizSucw"
		]
}</script>
    <meta name="theme-color" content="#478ac9">
    <meta name="twitter:site" content="@">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Post Template">
    <meta name="twitter:description" content="">
  </head>
  <body class="u-body u-xl-mode" data-lang="en"><header class=" u-clearfix u-header u-section-row-container" id="sec-b8de"><div class="u-section-rows">
        <div class="u-clearfix u-palette-2-light-1 u-section-row u-section-row-1" data-animation-name="" data-animation-duration="0" data-animation-delay="0" data-animation-direction="" id="sec-f7ac">
          <div class="u-clearfix u-sheet u-valign-middle-lg u-valign-middle-md u-valign-middle-sm u-valign-middle-xl u-sheet-1">
            <a href="../Home.html" data-page-id="208602995" class="u-image u-logo u-image-1" data-image-width="1000" data-image-height="1000" title="Home">
              <img src="../images/owanlogo.png" class="u-logo-image u-logo-image-1">
            </a>
            <nav class="u-menu u-menu-dropdown u-offcanvas u-menu-1">
              <div class="menu-collapse" style="font-size: 1rem; letter-spacing: 0px; font-weight: 700;">
                <a class="u-button-style u-custom-left-right-menu-spacing u-custom-padding-bottom u-custom-top-bottom-menu-spacing u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="#" style="padding: 4px 42px; font-size: calc(1em + 8px);">
                  <svg class="u-svg-link" viewBox="0 0 24 24"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#menu-hamburger"></use></svg>
                  <svg class="u-svg-content" version="1.1" id="menu-hamburger" viewBox="0 0 16 16" x="0px" y="0px" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://www.w3.org/2000/svg"><g><rect y="1" width="16" height="2"></rect><rect y="7" width="16" height="2"></rect><rect y="13" width="16" height="2"></rect>
</g></svg>
                </a>
              </div>
              <div class="u-custom-menu u-nav-container">
                <ul class="u-nav u-unstyled u-nav-1"><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="../Home.html" style="padding: 10px 20px;">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="../project-and-code.html" style="padding: 10px 20px;">project and code</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link u-text-active-palette-1-base u-text-hover-palette-2-base" href="../lecture.html" style="padding: 10px 5px 10px 20px;">lecture</a>
</li></ul>
              </div>
              <div class="u-custom-menu u-nav-container-collapse">
                <div class="u-black u-container-style u-inner-container-layout u-opacity u-opacity-95 u-sidenav">
                  <div class="u-inner-container-layout u-sidenav-overflow">
                    <div class="u-menu-close"></div>
                    <ul class="u-align-center u-nav u-popupmenu-items u-unstyled u-nav-2"><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../Home.html">Home</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../project-and-code.html">project and code</a>
</li><li class="u-nav-item"><a class="u-button-style u-nav-link" href="../lecture.html">lecture</a>
</li></ul>
                  </div>
                </div>
                <div class="u-black u-menu-overlay u-opacity u-opacity-70"></div>
              </div>
            </nav>
          </div>
          
          
          
          
          
        </div>
        <div class="u-section-row u-section-row-2" id="sec-e839">
          <div class="u-clearfix u-sheet u-sheet-2"></div>
          
          
          
          
          
        </div>
      </div></header>
    <section class="u-align-center u-clearfix u-section-1" id="sec-bf54">
      <div class="u-clearfix u-sheet u-valign-middle-md u-valign-middle-sm u-valign-middle-xs u-sheet-1"><!--post_details--><!--post_details_options_json--><!--{"source":""}--><!--/post_details_options_json--><!--blog_post-->
        <div class="u-container-style u-expanded-width u-post-details u-post-details-1">
          <div class="u-container-layout u-valign-middle u-container-layout-1", style= "position: center;padding: 0px 100px 5px 100px;"><!--blog_post_image-->
            <!--/blog_post_image--><!--blog_post_header-->
            <h2 class="u-blog-control u-text u-text-1">Support Vector Machine</h2><!--/blog_post_header--><!--blog_post_metadata-->
            <div class="u-blog-control u-metadata u-metadata-1"><!--blog_post_metadata_date-->
              <span class="u-meta-date u-meta-icon">Aug 13, 2022</span><!--/blog_post_metadata_date--><!--blog_post_metadata_category-->
              <!--/blog_post_metadata_category--><!--blog_post_metadata_comments-->
              <!--/blog_post_metadata_comments-->
            </div><!--/blog_post_metadata--><!--blog_post_content-->
            <div class="u-align-justify u-blog-control u-post-content u-text u-text-2 fr-view">
  
  </div>
<div style= "position: center;">
<p>
SVM solves the binary classification problem where we intead to find mapping \( f:\mathbb{R}^{D}\rightarrow \{+1,-1\}^{2}\). In contrast to other probabilistic approach that solve by maximum likelihood or Bayesian inference approach, SVM reason the problem geometrically. This relied on projection, inner product and constraint optimization. In SVM, we think of data point is seperated by hyperplane. This seperating hyperplane is an affine subspace of \( D-1\) for \( D\) dimensional features vector for example, in 2-dimensional space, the hyperplane is just a line. We first consider a dataset that is linearly seperable, e.g. data not have complex representation of features. It has infinite line to choose that solve this problem without an error. SVM find such line by maximizing the margin between pos+ and neg- class. The hyperplane define as \( \{x\in D:f( x) =0\}\), where the vector \( \omega \) is orthogonal to hyperplane and any vector on hyperplane. By choosing any \( x_{0}\) and \( x_{1}\), we have that \( f( x_{0}) -f( x_{1}) =< \omega ,x_{0} -x_{1}> =0\).

<p><center><img src="image/svm01.png" alt="lower bound", style="width:600px;height:160px;"></center></p>
SVM classify example based on by \( +1\) if \( f\geqslant 0\) and \( -1\) otherwise. Introduce variable \( y_{i}\) represent target label \( \{+1,\ -1\}\). The hyperplane seperate class by \( < \omega ,x_{i}> +b\geqslant 0\) when \( y_{i} =+1\) and \( < \omega ,x_{i}> +b< 0\) when \( y_{i} =-1\) where \( < \cdotp > \) is inner product operation. This can be written concisely as, by multiplying \( y_{i}\) in both side:
\begin{equation}
y_{i}(< \omega ,x_{i}> +b) \geqslant 0
\end{equation}
Note also that since \( \omega \) is of the same dimension with input \( x_{i}\) thus it the numbers of parameters in SVM grows only linearly with number of input features dimension.
</p>

<h4>Margin</h4>
<p>What is the distance of \( x_{i}\) and its orthogonal projection onto hyperplane?
<p><center><img src="image/svm02.png" alt="lower bound", style="width:600px;height:160px;"></center></p>
The vector \( r\) is also orthogonal to hyperplane and thus a scaled factor of vector \( \omega \). Then we have that the point \( x_{i}\) and \( x_{i} =x_{i}^{'} +r\alpha \omega \) for some scaling factor \( a\). For convenience, consier where \( \omega \) is unit vector then \( x_{i} =x_{i}^{'} +r\omega /\| \omega \| \), since only what we are interested in is the direction of \( \omega \) so \( r\) is also just scaled factor of this unit vector. And if \( x_{i}\) is the closest point to hyperplane then \( r\) is the distance of margin. We want that any examples must be greater than distance \( r\) from hyperplane \( y_{i}(< \omega ,x_{i}> +b) \geqslant r\). Note again that our assumption is \( \| \omega \| =1\). By working some algebra we have that 
\begin{gather}
\left< \omega ,x_{i}^{'} +r\omega /\| \omega \| \right> +b) =0\\
=< \omega ,x_{i}> -< \omega ,\omega > /\| \omega \| +b=0\\
=1/\| \omega \| 
\end{gather}
is the width of 1 side margin, since \( < \omega ,x_{i}> +b=1\) by assumption that any \( x_{i}\) lie within subspace spanned by \( \omega /\| \omega \| \) is \( 1\) (\( x_{i}\) lies exactly on the margin \( r\)). And that its projection lie on the hyperplane \( \left< \omega ,x_{i}^{'}\right> +b=0\). Recall that we want all sample to be have minimum distance from hyperplane of \( r\), similarly, we instead want any sample to by at least 1 distance away from hyperplane which yield the constrant that \( y_{i}(< \omega ,x_{i}> +b) \geqslant 1\) Finally, we can formulate SVM as constrained optimization as:
\begin{gather}
\underset{\omega ,b}{\mathbf{max}} \ 1/\| \omega \| \ \\
\mathbf{subject\ to\ } y_{i}(< \omega ,x_{i}> +b) \geqslant 1\ \forall _{i}
\end{gather}
This is also equivalent to maximize width of magin for both side \( 2/\| \omega \| \). In general, we consider instead of maximizing the margin by minimizing the squared norm \( 1/2\| \omega \| ^{2}\) with similar constrant and additional constant which tiding the function when computing the gradient of objective. Note that, in contrast to where I first say that we want to maximizing margin \( r\), it is equivalent to scaled data such that the closest point \( x_{i}\) has the distance of 1 to the hyperplane. For more detailed explanation for why it is equivalent, see ML book page 378. For simple explanation, consider if we would to scale \( \omega ,b\) by factor of \( 2\) and that the prediction \( f( \omega ,b) =f( 2\omega ,2b)\) would not change the predictor \( f\) at all. So it only depend on the sign not the magnitude of \( w,b\). So we can arbitrarily scale the parameters \( \omega ,b\) without changing anything else.
</p>
<h4>Soft Margin</h4>
<p>
In real world datasets, it is mostly that the data having the same label would lie in the space that the hyperplane could not seperate. Soft margin allow the constraint (6) to be violated by introducing slack variable \( \zeta _{i}\) for each of example \( x_{i}\) by subtracting \( \zeta _{i}\) from margin and is non-negative. The problem can then be  reformulated as 
\begin{gather}
\underset{\omega ,b}{\mathbf{min}} \ \frac{1}{2} \| \omega \| ^{2} \ +C\sum _{n=1}^{N} \zeta _{i}\\
\mathbf{subject\ to\ } y_{i}(< \omega ,x_{i}> +b) \geqslant 1-\zeta _{i} ,\ \zeta _{i} \geqslant 0\ \forall _{i}
\end{gather}
Here I replace objective with squared norm \( 1/2\| \omega \| ^{2}\). Like previous hard margin, soft margin has convex quadratic objective and linear constraints which its solution can be found by solving traditional quadratic programming. For more intuition why we our objective is to maximizing the margin \( r\), consider when \( y_{i} =+1\) is in positive class then we expect \( y_{i}(< \omega ,x_{i}> +b)\) to be large positive number and thus give more confident about the prediction. However, if the predictor would predict the same sample correctly, e.g. \( y_{i} =+1\), but very close to margin then we would have low confident about prediction. 
</p>
<h4>Dual Problem</h4>
<p>
We form the langragian by having (5) as a primal problem. Recall that in order to obtain equivalent dual problem, first we construct the Lagragian \( L( x,\lambda ,\upsilon )\) and the dual variables \( \lambda ,\upsilon \) is non-negative to ensure strong duality. Take the gradient with respect to \( x\), \( \nabla _{x} L\) set to zero and solve for \( x^{*}\). Substitute \( x^{*}\) back in to Lagragian \( L\) to obtain dual function \( g( \lambda ,\ \upsilon )\) which is the function of dual variables \( \lambda ,\upsilon \) only. Finally construct the dual with dual function and new constraints. We form the generalized Lagragian as 
\begin{gather}
L( \omega ,b,\lambda ) =\frac{1}{2} \| \omega \| ^{2} -\sum _{i} \lambda _{i}( y_{i}(< \omega ,x_{i}> +b) -1)\\
\partial L( \omega ,b,\lambda ) /\partial \omega =\omega -\sum _{i} \lambda _{i} y_{i} x_{i} =0,\ \omega ^{*} =\sum _{i} \lambda _{i} y_{i} x_{i}\\
\partial L( \omega ,b,\lambda ) /\partial b=-\sum _{i} \lambda _{i} y_{i} =0,\ b^{*} =\sum _{i} \lambda _{i} y_{i}
\end{gather}
Plugging \( \ \omega ^{*}\) and \( b^{*}\) back to the Lagragian \( L\) and write the sum explicitly as 
\begin{gather}
L=\frac{1}{2}\sum _{i} \lambda _{i} y_{i} x_{i}\sum _{j} \lambda _{j} y_{j} x_{j} -\sum _{i} \lambda _{i} y_{i} x_{i}\sum _{j} \lambda _{j} y_{j} x_{j} -\sum _{i} \lambda _{i} y_{i} +\sum _{i} \lambda _{i}\\
=-\frac{1}{2}\sum _{i}\sum _{j} \lambda _{i} \lambda _{j} y_{i} y_{j}< x_{i} ,x_{j}> -0+\sum _{i} \lambda _{i}\\
=\sum _{i} \lambda _{i} -\frac{1}{2}\sum _{i,j} \lambda _{i} \lambda _{j} y_{i} y_{j}< x_{i} ,x_{j}> 
\end{gather}
We then have the dual optimization problem:
\begin{equation}
\underset{\lambda }{\mathbf{max}}\sum _{i} \lambda _{i} -\frac{1}{2} \Lambda ^{T}\mathbb{H} \Lambda \ \mathbf{subject\ to} \ \lambda _{i} \geqslant 0\ \forall _{i} ,\ \sum _{i} \lambda _{i} y_{i} =0
\end{equation}
where \( \mathbb{H} =y_{i} y_{j}< x_{i} ,x_{j}> \) and concatenate \( \lambda _{i}\) into vector to obtain \( \Lambda \). The optimal \( \omega ,\ b\) in primal is just the linear combination of input ventor \( x_{i}\) and its solution is spanned by training datasets. This dual form only require the dot product for each example to be calculated and is important for the use of kernel trick in nonlinearity in feature space. This dual problem can be verified that the dual gap is zero and satisfied Karush-Kuhn-Tucker conditions. Now if we have the optimal value of \( \omega \) in terms of \( \lambda \), then to make the prediction by plugging in the equation (10) to original \( < \omega ,x_{i}> +b\). And predict that \( y_{i} =+1\) if \( < \omega ,x_{i}> +b >0\). 
\begin{equation}
< \omega ,x_{k}> +b=\left< \sum _{i} \lambda _{i} y_{i} x_{i} ,x_{k}\right> +b=\sum _{i} \lambda _{i} y< x_{i} ,x_{k}> +b
\end{equation}
The input vector \( x_{i}\) for which corresponding parameters \( \lambda _{i}\) is zero, do not contribute to the solution of \( w\) at all. The input vector \( x_{i}\) where \( \lambda _{i} =0\) is supporting vector since these example "support" the hyperplane thus most inputs vector of the sum of equation (16) will be zero. In addition to soft margin, we can also calculate the gradient as previously as \( \partial L( \omega ,b,\lambda ) /\partial \zeta _{i} =C-\lambda _{i} -\zeta _{i}\). And adding this to the Langragian will vanish to zero. Only additional modification is, since the dual variable is non-negative, we only need to add the second constraint to (15) by \( 0\leqslant \lambda _{k} \leqslant C\) for all \( i=1,...N\). 

To use python implementation for solving convex optimization, in this case I use CVXOPT, first we need to write (9) as (it is required by CVXOPT):
\begin{equation}
\mathbf{min} \ ( 1/2) \Lambda ^{T}\mathbb{H} \Lambda -1^{T} \lambda \ \mathbf{subject\ to} \ y^{T} \Lambda =0,\ -\lambda _{i} \leqslant 0\ \forall _{i}
\end{equation}
For full implementation can be found in my subsequence post.
</p>
<p>
TODO :
kernel method
Ref probabilistic machine learning P. 579
Ref Andrew Ng lecture notes on SVM
Ref Tristan Fletcher lecture notes on SVM
Implementation using CVXOPT library
More on quadratic programming
</p>
</div>
          </div>
        </div><!--/blog_post--><!--/post_details-->
      </div>
    </section>
    
    
    <footer class="u-clearfix u-footer u-grey-80" id="sec-e381"><div class="u-clearfix u-sheet u-sheet-1">
        <div class="u-align-left u-social-icons u-spacing-10 u-social-icons-1">
          <a class="u-social-url" title="facebook" target="_blank" href="https://www.facebook.com/owan.naruemit"><span class="u-icon u-social-facebook u-social-icon u-icon-1"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-9909"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-9909"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M73.5,31.6h-9.1c-1.4,0-3.6,0.8-3.6,3.9v8.5h12.6L72,58.3H60.8v40.8H43.9V58.3h-8V43.9h8v-9.2
            c0-6.7,3.1-17,17-17h12.5v13.9H73.5z"></path></svg></span>
          </a>
          <a class="u-social-url" title="twitter" target="_blank" href="https://twitter.com/napanaruemit"><span class="u-icon u-social-icon u-social-twitter u-icon-2"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-d07a"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-d07a"><circle fill="currentColor" class="st0" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M83.8,47.3c0,0.6,0,1.2,0,1.7c0,17.7-13.5,38.2-38.2,38.2C38,87.2,31,85,25,81.2c1,0.1,2.1,0.2,3.2,0.2
            c6.3,0,12.1-2.1,16.7-5.7c-5.9-0.1-10.8-4-12.5-9.3c0.8,0.2,1.7,0.2,2.5,0.2c1.2,0,2.4-0.2,3.5-0.5c-6.1-1.2-10.8-6.7-10.8-13.1
            c0-0.1,0-0.1,0-0.2c1.8,1,3.9,1.6,6.1,1.7c-3.6-2.4-6-6.5-6-11.2c0-2.5,0.7-4.8,1.8-6.7c6.6,8.1,16.5,13.5,27.6,14
            c-0.2-1-0.3-2-0.3-3.1c0-7.4,6-13.4,13.4-13.4c3.9,0,7.3,1.6,9.8,4.2c3.1-0.6,5.9-1.7,8.5-3.3c-1,3.1-3.1,5.8-5.9,7.4
            c2.7-0.3,5.3-1,7.7-2.1C88.7,43,86.4,45.4,83.8,47.3z"></path></svg></span>
          </a>
          <a class="u-social-url" title="linkedin" target="_blank" href="https://www.linkedin.com/in/pethai-napanaruemit-3a69a7190/"><span class="u-icon u-social-icon u-social-linkedin u-icon-3"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-694a"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-694a"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M41.3,83.7H27.9V43.4h13.4V83.7z M34.6,37.9L34.6,37.9c-4.6,0-7.5-3.1-7.5-7c0-4,3-7,7.6-7s7.4,3,7.5,7
            C42.2,34.8,39.2,37.9,34.6,37.9z M89.6,83.7H76.2V62.2c0-5.4-1.9-9.1-6.8-9.1c-3.7,0-5.9,2.5-6.9,4.9c-0.4,0.9-0.4,2.1-0.4,3.3v22.5
            H48.7c0,0,0.2-36.5,0-40.3h13.4v5.7c1.8-2.7,5-6.7,12.1-6.7c8.8,0,15.4,5.8,15.4,18.1V83.7z"></path></svg></span>
          </a>
          <a class="u-social-url" target="_blank" data-type="Github" title="Github" href="https://github.com/pethai2004"><span class="u-icon u-social-github u-social-icon u-icon-4"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-a39c"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-a39c"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M88,51.3c0-5.5-1.9-10.2-5.3-13.7c0.6-1.3,2.3-6.5-0.5-13.5c0,0-4.2-1.4-14,5.3c-4.1-1.1-8.4-1.7-12.7-1.8
	c-4.3,0-8.7,0.6-12.7,1.8c-9.7-6.6-14-5.3-14-5.3c-2.8,7-1,12.2-0.5,13.5C25,41.2,23,45.7,23,51.3c0,19.6,11.9,23.9,23.3,25.2
	c-1.5,1.3-2.8,3.5-3.2,6.8c-3,1.3-10.2,3.6-14.9-4.3c0,0-2.7-4.9-7.8-5.3c0,0-5-0.1-0.4,3.1c0,0,3.3,1.6,5.6,7.5c0,0,3,9.1,17.2,6
	c0,4.3,0.1,8.3,0.1,9.5h25.2c0-1.7,0.1-7.2,0.1-14c0-4.7-1.7-7.9-3.4-9.4C76,75.2,88,70.9,88,51.3z"></path></svg></span>
          </a>
          <a class="u-social-url" target="_blank" data-type="YouTube" title="YouTube" href="https://www.youtube.com/channel/UCkQZQFB-PPBgNEL-GizSucw"><span class="u-icon u-social-icon u-social-youtube u-icon-5"><svg class="u-svg-link" preserveAspectRatio="xMidYMin slice" viewBox="0 0 112 112" style=""><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#svg-7985"></use></svg><svg class="u-svg-content" viewBox="0 0 112 112" x="0" y="0" id="svg-7985"><circle fill="currentColor" cx="56.1" cy="56.1" r="55"></circle><path fill="#FFFFFF" d="M74.9,33.3H37.3c-7.4,0-13.4,6-13.4,13.4v18.8c0,7.4,6,13.4,13.4,13.4h37.6c7.4,0,13.4-6,13.4-13.4V46.7 C88.3,39.3,82.3,33.3,74.9,33.3L74.9,33.3z M65.9,57l-17.6,8.4c-0.5,0.2-1-0.1-1-0.6V47.5c0-0.5,0.6-0.9,1-0.6l17.6,8.9 C66.4,56,66.4,56.8,65.9,57L65.9,57z"></path></svg></span>
          </a>
        </div>
        <div class="u-border-1 u-border-white u-expanded-width u-line u-line-horizontal u-opacity u-opacity-50 u-line-1"></div>
      </div></footer>
    <section class="u-backlink u-clearfix u-grey-80">
      <a class="u-link" href="https://nicepage.com/website-templates" target="_blank">
        <span>Website Templates</span>
      </a>
      <p class="u-text">
        <span>created with</span>
      </p>
      <a class="u-link" href="" target="_blank">
        <span>Website Builder Software</span>
      </a>. 
    </section>
  
</body></html>